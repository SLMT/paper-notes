<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Paper Notes</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
                <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="cover.html">Cover</a></li><li class="chapter-item expanded affix "><a href="template.html">Template</a></li><li class="spacer"></li><li class="chapter-item expanded "><a href="dbms_machine_provisioning/summary.html"><strong aria-hidden="true">1.</strong> DBMS Workload Modeling &amp; Machine Provisioning</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dbms_machine_provisioning/sheikh2011bayesian.html"><strong aria-hidden="true">1.1.</strong> ICAC'11 - Modeling Workloads using Gaussian Process</a></li><li class="chapter-item expanded "><a href="dbms_machine_provisioning/taft2018pstore.html"><strong aria-hidden="true">1.2.</strong> SIGMOD'18 - P-Store</a></li></ol></li><li class="chapter-item expanded "><a href="dbms_data_partitioning/summary.html"><strong aria-hidden="true">2.</strong> DBMS Data Partitioning</a></li><li class="chapter-item expanded "><a href="dbms_query_processing/summary.html"><strong aria-hidden="true">3.</strong> DBMS Query Processing</a></li><li class="chapter-item expanded "><a href="dbms_scalability/summary.html"><strong aria-hidden="true">4.</strong> DBMS Scalability</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dbms_scalability/lu2019star.html"><strong aria-hidden="true">4.1.</strong> VLDB'19 - Star</a></li><li class="chapter-item expanded "><a href="dbms_scalability/georgiou2020hihooi.html"><strong aria-hidden="true">4.2.</strong> TKDE'20 - Hihooi</a></li></ol></li><li class="chapter-item expanded "><a href="deterministic_dbms/summary.html"><strong aria-hidden="true">5.</strong> Deterministic DBMS</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="deterministic_dbms/ren2014evaluation.html"><strong aria-hidden="true">5.1.</strong> VLDB'14 - Advantages and Disadvantages of Deterministic DBMS</a></li><li class="chapter-item expanded "><a href="deterministic_dbms/lu2020aria.html"><strong aria-hidden="true">5.2.</strong> VLDB'20 - Aria</a></li><li class="chapter-item expanded "><a href="deterministic_dbms/liu2022snapper.html"><strong aria-hidden="true">5.3.</strong> SIGMOD'22 - Snapper</a></li></ol></li><li class="chapter-item expanded "><a href="dbms_with_ai/summary.html"><strong aria-hidden="true">6.</strong> DBMS + AI</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dbms_with_ai/aken2017ottertune.html"><strong aria-hidden="true">6.1.</strong> SIGMOD'17 - OtterTune</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/marcus2019dlquery.html"><strong aria-hidden="true">6.2.</strong> CIDR'19 - Query Optimizer through DL</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/zhou2020dbaisurvey.html"><strong aria-hidden="true">6.3.</strong> TKDE'20 - Database Meets AI: A Survey</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/lin2021mb2.html"><strong aria-hidden="true">6.4.</strong> SIGMOD'21 - MB2</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/sioulas2021roulette.html"><strong aria-hidden="true">6.5.</strong> SIGMOD'21 - Scalable Multi-Query Execution using Reinforcement Learning</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/cereda2021cgptuner.html"><strong aria-hidden="true">6.6.</strong> VLDB'21 - CGPTuner</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/hilprecht2022zeroshot.html"><strong aria-hidden="true">6.7.</strong> CIDR'22 - Zero-Shot Learning on DBMS</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/yang2022balsa.html"><strong aria-hidden="true">6.8.</strong> SIGMOD'22 - Balsa</a></li><li class="chapter-item expanded "><a href="dbms_with_ai/butrovich2022tscout.html"><strong aria-hidden="true">6.9.</strong> SIGMOD'22 - TScout</a></li></ol></li><li class="chapter-item expanded "><a href="dbms_experiments/summary.html"><strong aria-hidden="true">7.</strong> DBMS Experiments</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="dbms_experiments/wang2022sensitivity.html"><strong aria-hidden="true">7.1.</strong> VLDB'22 - A Study of Database Performance Sensitivity to Experiment Settings</a></li></ol></li><li class="chapter-item expanded "><a href="rl_to_rank/summary.html"><strong aria-hidden="true">8.</strong> Reinforcement Learning to Rank</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="rl_to_rank/zhou2020rlirank.html"><strong aria-hidden="true">8.1.</strong> WWW'20 - RLIRank</a></li></ol></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">Paper Notes</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="paper-notes"><a class="header" href="#paper-notes">Paper Notes</a></h1>
<p>This is a collection of paper notes written by <a href="https://www.slmt.tw">Yu-Shan Lin</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="template"><a class="header" href="#template">Template</a></h1>
<ul>
<li>Authors: </li>
<li>Institute: </li>
<li>Published at </li>
<li>Paper Link: </li>
</ul>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<h2 id="motivation"><a class="header" href="#motivation">Motivation</a></h2>
<h2 id="problem"><a class="header" href="#problem">Problem</a></h2>
<h2 id="method"><a class="header" href="#method">Method</a></h2>
<h2 id="experiments"><a class="header" href="#experiments">Experiments</a></h2>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<h2 id="questions"><a class="header" href="#questions">Questions</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dbms-machine-provisioning"><a class="header" href="#dbms-machine-provisioning">DBMS Machine Provisioning</a></h1>
<ul>
<li><a href="dbms_machine_provisioning/sheikh2011bayesian.html">ICAC'11 - A Bayesian Approach to Online Performance Modeling for Database Appliances using Gaussian Models</a>
<ul>
<li>Problem: modeling DBMS workloads using Gaussian Process</li>
<li>Keys:
<ul>
<li>It only focuses on modeling workloads and does not propose any application.</li>
<li>It demonstrate GP can predict quite accurate with small data set.</li>
</ul>
</li>
</ul>
</li>
<li><a href="dbms_machine_provisioning/taft2018pstore.html">SIGMOD'18 - P-Store: An Elastic Database System with Predictive Provisioning</a>
<ul>
<li>Motivation: previous work always react after an overloaded event happens.</li>
<li>Problem: they propose to make machine provisioning decision by predicting the following workloads.</li>
<li>Keys:
<ul>
<li>The target workload must be easy to predict.</li>
<li>There can be a few distributed transactions.</li>
</ul>
</li>
</ul>
</li>
<li>IEEE CloudCom'18 - DERP: A Deep Reinforcement Learning Cloud System for Elastic Resource Provisioning
<ul>
<li><a href="https://ieeexplore.ieee.org/abstract/document/8590989">https://ieeexplore.ieee.org/abstract/document/8590989</a></li>
<li>Motivation: previous work can not deal with large input space so we need a learning based method.</li>
<li>Method: Uses a DQN RL agent to decide when to add/remove machines and how many machines are added/removed to a DBMS cluster.</li>
<li>Key Points:
<ul>
<li>Targeting on NoSQL systems.</li>
</ul>
</li>
</ul>
</li>
<li>VLDB'19 - iBTune: individualized buffer tuning for large-scale cloud databases
<ul>
<li><a href="https://dl.acm.org/doi/abs/10.14778/3339490.3339503">https://dl.acm.org/doi/abs/10.14778/3339490.3339503</a></li>
<li>Focus on tuning buffer size</li>
</ul>
</li>
<li>VLDB'21 - Seagull: An Infrastructure for Load Prediction andOptimized Resource Allocation
<ul>
<li><a href="http://www.vldb.org/pvldb/vol14/p154-poppe.pdf">http://www.vldb.org/pvldb/vol14/p154-poppe.pdf</a></li>
<li>Problem: to predict the load of a DBMS server and use the predicted info to decide when to backup the DB.</li>
<li>Keys:
<ul>
<li>Focuses on system design</li>
<li>Assumes the target workload have periodical patterns</li>
<li>Tried methods to predict workloads
<ul>
<li>Singular Spectrum Analysis</li>
<li>Feed-forward Networks</li>
<li>Prophet: a software with a model proposed by Facebook to predict time series data with yearly, weekly, and daily patterns.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-bayesian-approach-to-online-performance-modeling-for-database-appliances-using-gaussian-models"><a class="header" href="#a-bayesian-approach-to-online-performance-modeling-for-database-appliances-using-gaussian-models">A Bayesian Approach to Online Performance Modeling for Database Appliances using Gaussian Models</a></h1>
<ul>
<li>Authors: Muhammad Bilal Sheikh, Umar Farooq Minhas, Omar Zia Khan, Ashraf Aboulnaga, Pascal Poupart, David J Taylor</li>
<li>Institute: University of Waterloo</li>
<li>Published at ICAC'11</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/10.1145/1998582.1998603">https://dl.acm.org/doi/10.1145/1998582.1998603</a></li>
</ul>
<h2 id="background-1"><a class="header" href="#background-1">Background</a></h2>
<ul>
<li>Database Appliance
<ul>
<li>A VM with a pre-installed copy of a OS and a DBMS</li>
<li>Easy to deploy</li>
</ul>
</li>
</ul>
<h2 id="motivation-1"><a class="header" href="#motivation-1">Motivation</a></h2>
<ul>
<li>DBA may need to predict workloads to decide how to allocate resources</li>
<li>Previous work on this
<ul>
<li>Analytical models
<ul>
<li>Need a domain expert</li>
<li>Specific to a particular DBMS</li>
</ul>
</li>
<li>Experiment-driven
<ul>
<li>Method
<ul>
<li>Modeling workloads by sampling from query executions</li>
<li>Use statistical models to fit the workloads</li>
</ul>
</li>
<li>Problems
<ul>
<li>Any new change to the workloads make these previous methods need to collect new data and retrain their models.</li>
<li>Hard to introduce prior knowledge to the models.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="problem-1"><a class="header" href="#problem-1">Problem</a></h2>
<p>To model workloads with Gaussian Process and make it adapt to changing workloads fast.</p>
<h3 id="formal-definition"><a class="header" href="#formal-definition">Formal Definition</a></h3>
<p>Assumptions:</p>
<ul>
<li>Each query belongs to a particular query type \(Q_i\), where \(1 \le i \le T\).</li>
<li>There are \(T\) types of queries.</li>
<li>A mix of queries \(m_j\) is represented as a vector \(&lt;N_{1j},...,N_{Tj}&gt;\), where \(N_{ij}\) represents # of queries in type \(Q_i\).</li>
<li>The total number of queries in a mix is less than \(M\), where \(M\) is defined by the DBA.</li>
<li>The samples for the mix \(m_j\) is represented as \(S_j = &lt;m_j,r_{ij}&gt;\) where \(r_{ij}\) is the real response time for a query in type \(Q_i\) in mix \(m_j\).</li>
</ul>
<p>Goal:</p>
<p>To find a function \(f(.)\) such that \( \hat{r}_{ij} = f(m_j, Q_i) \) where \( \hat{r}_{ij} \) is the estimated response time for a query in type \(Q_i\).</p>
<h2 id="method-1"><a class="header" href="#method-1">Method</a></h2>
<h3 id="main-idea"><a class="header" href="#main-idea">Main Idea</a></h3>
<p>It maintains two models:</p>
<ul>
<li>Response Time Model
<ul>
<li>Given the current workload mix \(m_i\) and the target query type \(Q_i\), predict the response time.</li>
</ul>
</li>
<li>Configuration Model
<ul>
<li>Given the current system configuration, predict the parameters of the response time model.</li>
</ul>
</li>
</ul>
<p>With these models, the system will not need to retrain for new system configs because it can predict the parameters from the configuration model.</p>
<h3 id="system-overview"><a class="header" href="#system-overview">System Overview</a></h3>
<p><img src="https://i.imgur.com/8jmV23I.png" alt="" /></p>
<h3 id="each-components"><a class="header" href="#each-components">Each Components</a></h3>
<h4 id="generating-training-data"><a class="header" href="#generating-training-data">Generating Training Data</a></h4>
<p>Two ways:</p>
<ul>
<li>Uniformly sampling # of queries for each query type
<ul>
<li>This will generating a data set with a small variance and its total load would concentrate on \(\frac{M}{2}\). Not good for learning.</li>
</ul>
</li>
<li>Uniformly sampling (the total number of queries, the number of different types of queries)</li>
</ul>
<h4 id="modeling-response-time"><a class="header" href="#modeling-response-time">Modeling Response Time</a></h4>
<p>Proposed Two Types of Models</p>
<ul>
<li>Linear Gaussian Models
<ul>
<li>Input: could be
<ol>
<li>the current total load (# of queries) \(l\)<br />
=&gt; Linear Load Model</li>
<li>the # of queries for each query type \(m = &lt;N_{1},...,N_{T}&gt;\)<br />
=&gt; Linear Query Mix Model</li>
</ol>
</li>
<li>Output: the response time \(r\)</li>
<li>Model: \(P(r|l;\theta) = \mathcal{N}(\beta_0 + \beta_1 l, \sigma^2)\)</li>
<li>How to learn? Maximum Likelihood Estimation (MLE)</li>
</ul>
</li>
<li>Gaussian Process Models
<ul>
<li>Input: could be
<ol>
<li>the current total load (# of queries) \(l\)<br />
=&gt; Gaussain Process Load Model (GPLM)</li>
<li>the # of queries for each query type \(m = &lt;N_{1},...,N_{T}&gt;\)<br />
=&gt; Gaussian Process Mix Model (GPMM)</li>
<li>Combination of total load \(l\) and mix \(m\)<br />
=&gt; Gaussian Process Mix + Load Model (GPMLM)</li>
</ol>
</li>
<li>Output: a gaussian distribution of the response time \(r\)</li>
<li>Model: Gaussain Process
<ul>
<li>Mean Functions:
<ol>
<li>0 mean</li>
<li>linear mean function: \(mean(x) = \beta_0 + \beta_1 x_1 + ... + \beta_T x_T\)</li>
</ol>
</li>
<li>Kernel Functions:
<ol>
<li>Squared Exponential Function (SE, i.e. RBF Kernel)<br />
$$
k(x, x') = \sigma^2 exp(\frac{-||x - x'||^2}{2 \eta ^ 2 I})
$$</li>
<li>Rational Quadratic Function (RQ)
$$
k(x, x') = \sigma^2 [1 + \frac{||x - x'||^2}{2 \alpha \eta ^ 2 I}] ^ {-\alpha}
$$</li>
</ol>
</li>
</ul>
</li>
<li>How to find the hyper-parameters? Same as linear models, Maximum Likelihood Estimation (MLE).</li>
</ul>
</li>
</ul>
<h4 id="modeling-hyper-parameters-of-a-response-time-model"><a class="header" href="#modeling-hyper-parameters-of-a-response-time-model">Modeling Hyper-parameters of a Response Time Model</a></h4>
<p>They found that</p>
<ul>
<li>A different configuration of the system needs a different set of hyper-parameters (i.e. a different model)</li>
<li>If a configuration do not appear in the training data set, the model may not learn well.</li>
</ul>
<p>So, we need a model to predict hyper-parameters for GP models.</p>
<ul>
<li>Input:
<ul>
<li>Mean of recent response time: \(R_{MEAN}\)</li>
<li>STD of recent response time: \(R_{SD}\)</li>
<li>Buffer Pool Size: \(BP\)</li>
<li>CPU Count: \(CPU_{NUM}\)</li>
<li>CPU Frequency (in MHz): \(CPU\)</li>
<li>Memory Size: \(MEM\)</li>
</ul>
</li>
<li>Output: each hyper-parameter used by GP models (one model per hyper-parameter)</li>
<li>Model: should be Gaussain Process, but the paper does not say explictly
<ul>
<li>Mean and kernel functions are unknown.</li>
</ul>
</li>
</ul>
<h2 id="experiments-1"><a class="header" href="#experiments-1">Experiments</a></h2>
<h3 id="model-accuracy"><a class="header" href="#model-accuracy">Model Accuracy</a></h3>
<p>Effect of Buffer Pool Size (Figure 3)</p>
<ul>
<li>It shows the linear models work poorer than GP models in all conditions, especially when the database fit partially in the buffer pool.</li>
</ul>
<p>Effectiveness under overload (Figure 4)</p>
<ul>
<li>It shows the GP models able to capture the variance of response time even if the system is overloaded and the variance is large.</li>
</ul>
<p>Overall Accuracy (Figure 5)</p>
<ul>
<li>It shows GPMLM (0, RQ) works well in all tests.</li>
</ul>
<h3 id="online-adaptability"><a class="header" href="#online-adaptability">Online Adaptability</a></h3>
<p>Online Costs</p>
<ul>
<li>Linear models work poorly so it does not adapt it to the online setting.</li>
<li>GP models with linear mean have very high cost since the mean function has \(T + 1\) hyper-parameters to learn.
<ul>
<li>Takes 1 hour to learn for 22 query types with 500 samples/type.</li>
</ul>
</li>
<li>GP models with 0 mean and RQ kernel works best.
<ul>
<li>Takes 4~7 minutes to learn for 22 query types with 500 samples/type.</li>
</ul>
</li>
</ul>
<p>Adapting to Dynamic Configurations (Figure 6)</p>
<ul>
<li>This experiment evaluates how the model performs when the configuration changes.</li>
<li>If each time the configuration changes and the model simply throw all samples, the results show it suffer from high error rate in the beginning.</li>
<li>If the model does not throw the samples but keeps them, the results show the error rate would be much lower at the same time.</li>
<li>The results also show that the online models work similar to the models pre-trained using the same workload.</li>
</ul>
<p>Adapting to Dynamic Workloads (Figure 7)</p>
<ul>
<li>This experiment evaluates how the model performs when new query types appear in the workload.</li>
<li>The model that keeps the old data while collecting new data works best.</li>
</ul>
<h4 id="model-robustness"><a class="header" href="#model-robustness">Model Robustness</a></h4>
<p>Impact of New Queries (Figure 8)</p>
<ul>
<li>GPMLM(0, RQ) works best with 4% increase in precentage error when there are 5 new query types.
<ul>
<li>This is because GPMLM also models the total number of queries which is a useful info for predicting response time.</li>
</ul>
</li>
</ul>
<p>Online Model Convergence (Figure 9)</p>
<ul>
<li>This experiment tests how well GPCM works when a new query type appear</li>
<li>Setting the hyper-parameters to 0 works worst.</li>
<li>Setting the hyper-parameters by averaging over existing parameters work quite well.
<ul>
<li>This shows that there are correlation between these parameters</li>
</ul>
</li>
<li>Setting the hyper-parameters using GPCM works best.</li>
<li>It also shows that 100 samples are enoguh for a good GP model.</li>
</ul>
<p>Configuration Model Accuarcy (Figure 10)</p>
<ul>
<li>I don't understand this...</li>
</ul>
<h3 id="notable-references"><a class="header" href="#notable-references">Notable References</a></h3>
<ul>
<li>Use Gaussian Process to model workloads
<ul>
<li>EDBT'11 - Predicting completion times of batch query workloads using interaction-aware models and simulation.</li>
<li>SIGMOD'10 - iTuned: a tool for configuring and visualizing database parameters.</li>
</ul>
</li>
</ul>
<h3 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h3>
<ul>
<li>Pros
<ul>
<li>GP's advantages
<ul>
<li>Can introduce prior knowledge (distributions)</li>
<li>Can provide confidence intervals for each prediction</li>
</ul>
</li>
</ul>
</li>
<li>Cons
<ul>
<li>It does not propose any application on DBMSs.</li>
</ul>
</li>
</ul>
<h3 id="questions-1"><a class="header" href="#questions-1">Questions</a></h3>
<ul>
<li>Why not just use online-learning methods to overcome dynamic workloads?</li>
<li>It seems like it assumes that every query in the same type has the same response time or at least similiar time. Is this a reasonable assumption?</li>
<li>How does the second way of sampling decides the ratio of number of queries between each type when generating training data?</li>
<li>What is the difference between the kernel functions that this paper uses?</li>
<li>Why is training the configuration model more reasonable than training a single response time model?
<ul>
<li>because the response time model can only work for one type of system configuration.</li>
<li>the space of system configurations is much smaller than the space of possible workloads.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="p-store-an-elastic-database-system-with-predictive-provisioning"><a class="header" href="#p-store-an-elastic-database-system-with-predictive-provisioning">P-Store: An Elastic Database System with Predictive Provisioning</a></h1>
<ul>
<li>Authors: Rebecca Taft, Nosayba El-Sayed, Marco Serafini, Yu Lu, Ashraf Aboulnaga, Michael Stonebraker, Ricardo Mayerhofer, Francisco Andrade</li>
<li>Institute: MIT, Qatar Computing Research Institute - HBKU, Urbana-Champaign, B2W Digital</li>
<li>Published at SIGMOD'18</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/abs/10.1145/3183713.3190650">https://dl.acm.org/doi/abs/10.1145/3183713.3190650</a></li>
</ul>
<h2 id="motivation-2"><a class="header" href="#motivation-2">Motivation</a></h2>
<p>DBMS machine provisioning is an important topic that controls the elasiticity and resource utilization of a distributed DBMS.</p>
<p>However, existing approaches always react slower than the actual demand.</p>
<h2 id="problem-2"><a class="header" href="#problem-2">Problem</a></h2>
<p>To design a system online reconfiguration strategy that reacts before the system overloaded such that:</p>
<ul>
<li>the resource used by the system is minimized</li>
<li>the reconfiguration does not violate SLA.</li>
</ul>
<p>Input:</p>
<ul>
<li>A prediction to the future workload</li>
<li>The capacity of a machine</li>
</ul>
<p>Output:</p>
<ul>
<li>When to add/remove machines</li>
<li>How many machines to be added or removed</li>
</ul>
<p>Assumptions:</p>
<ul>
<li>The target workload has periodic patterns that are easy to be predicted.</li>
<li>There is no spike in the workload.</li>
<li>Only a few distributed transactions.</li>
</ul>
<p>This goal can be visualized as follows:</p>
<p><img src="dbms_machine_provisioning/taft2018pstore-figure1.png" alt="P-Store Goal" /></p>
<h2 id="method-2"><a class="header" href="#method-2">Method</a></h2>
<p>Two Parts:</p>
<ul>
<li>Workload Prediction</li>
<li>Allocation Decision</li>
</ul>
<h3 id="workload-prediction"><a class="header" href="#workload-prediction">Workload Prediction</a></h3>
<p>Models the workload as a time series data and uses Sparse Periodic Auto Regression to predict [USENIX'08]. </p>
<p>Models the future workload at a time as a sum of a long-term pattern (past n days) and a short-term pattern (past m minutes).</p>
<h3 id="allocation-decision"><a class="header" href="#allocation-decision">Allocation Decision</a></h3>
<p>Use DP.</p>
<h2 id="comments"><a class="header" href="#comments">Comments</a></h2>
<ul>
<li>Pros
<ul>
<li>Works well on predictable workloads</li>
</ul>
</li>
<li>Cons
<ul>
<li>The workload must be easy to predict</li>
<li>The database must be easy to partition so that P-Store won't need to consider the impact of distributed transactions.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dbms-data-partitioning"><a class="header" href="#dbms-data-partitioning">DBMS Data Partitioning</a></h1>
<ul>
<li>PEN'19 - Using machine learning for intelligent shard sizing on the cloud
<ul>
<li><a href="http://pen.ius.edu.ba/index.php/pen/article/view/332">http://pen.ius.edu.ba/index.php/pen/article/view/332</a></li>
<li>Problem: decides data partitioning by predicting the latency of a DBMS application with a given data partitioning strategy.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dbms-query-processing"><a class="header" href="#dbms-query-processing">DBMS Query Processing</a></h1>
<ul>
<li>SIGMOD'20 - Thrifty Query Execution via Incrementability
<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3318464.3389756">https://dl.acm.org/doi/abs/10.1145/3318464.3389756</a></li>
<li>Problem: to study how to efficiently evaluate a query even before all the data are ready.
<ul>
<li>Then, the query can be executed faster when all data are set.</li>
</ul>
</li>
<li>Motivation: previous work only focus on select-project-join-aggregate queries, but not more complex queries such as nested queries and outer/anti-joins.</li>
<li>Assumption: data arrival rate can be predicted from historical statistics</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dbms-scalability"><a class="header" href="#dbms-scalability">DBMS Scalability</a></h1>
<ul>
<li><a href="dbms_scalability/lu2019star.html">VLDB'19 - STAR: Scaling Transactions through Asymmetric Replication</a></li>
<li><a href="dbms_scalability/georgiou2020hihooi.html">TKDE'20 - Hihooi: A Database Replication Middleware forScaling Transactional Databases Consistently</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="star-scaling-transactions-through-asymmetric-replication"><a class="header" href="#star-scaling-transactions-through-asymmetric-replication">STAR: Scaling Transactions through Asymmetric Replication</a></h1>
<ul>
<li>Authors: Yi Lu, Xiangyao Yu, Samuel Madden</li>
<li>Institute: MIT CSAIL</li>
<li>Published at VLDB'19</li>
<li>Paper Link: <a href="http://www.vldb.org/pvldb/vol12/p1316-lu.pdf">http://www.vldb.org/pvldb/vol12/p1316-lu.pdf</a></li>
</ul>
<h2 id="motivation-3"><a class="header" href="#motivation-3">Motivation</a></h2>
<p>Cross-partitions transactions hurt scalability of distributed database systems due to two-phase commit.</p>
<h2 id="problem-3"><a class="header" href="#problem-3">Problem</a></h2>
<p>To design a better execution scheme to avoid executing cross-partition transactions in a distributed way.</p>
<h3 id="assumptions"><a class="header" href="#assumptions">Assumptions</a></h3>
<ul>
<li>A partitioned distributed DBMS</li>
<li>One of the nodes has enough memory capacity for a complete replica.</li>
</ul>
<h2 id="method-3"><a class="header" href="#method-3">Method</a></h2>
<ol>
<li>Separate the transactions into two categories:
<ul>
<li>Single-partition transactions</li>
<li>Cross-partitions transactions</li>
</ul>
</li>
<li>Separate machines in the cluster into two categories:
<ul>
<li>Partial-replica machines</li>
<li>Full-replica machines</li>
</ul>
</li>
<li>Then, divide the execution into two phases:
<ul>
<li>Partitioned Phase
<ul>
<li>Executes only single-partition transactions.</li>
<li>Each partition has a partial-replica machine as its primary machine.</li>
<li>A thread takes the responsibility to execute single-partition transactions on a partition.</li>
</ul>
</li>
<li>Single-master Phase
<ul>
<li>Executes only cross-partition transactions.</li>
<li>A full-replica machine will be the master node for all the transactions.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="conclusion-2"><a class="header" href="#conclusion-2">Conclusion</a></h2>
<ul>
<li>Pros
<ul>
<li>Eliminates distributed transactions</li>
</ul>
</li>
<li>Cons
<ul>
<li>This method assumes that there is a machine which has high computing power to execute transactions and high memory capacity to store all the data in memory.</li>
<li>During phase transition, it requests all participants to synchronize with each others. This may be unrealistic for cross-WAN settings.
<ul>
<li>On the other hand, Calvin only needs a part of machines to reach a consensus and replicates inputs.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="questions-2"><a class="header" href="#questions-2">Questions</a></h2>
<ul>
<li>Q: How about deterministic DBMSs?
<ul>
<li>The paper argues that the total ordering for deterministic DBMSs is costly.</li>
<li>But, is the replication fence of STAR not costly?</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hihooi-a-database-replication-middleware-for-scaling-transactional-databases-consistently"><a class="header" href="#hihooi-a-database-replication-middleware-for-scaling-transactional-databases-consistently">Hihooi: A Database Replication Middleware for Scaling Transactional Databases Consistently</a></h1>
<ul>
<li>Authors: Michael A. Georgiou, Aristodemos Paphitis, Michael Sirivianos, Herodotos Herodotou</li>
<li>Institute: Cyprus University of Technology, Limassol, Cyprus</li>
<li>Published at TKDE'20</li>
<li>Paper Link: <a href="https://ieeexplore.ieee.org/abstract/document/9068420">https://ieeexplore.ieee.org/abstract/document/9068420</a></li>
</ul>
<h2 id="motivation-4"><a class="header" href="#motivation-4">Motivation</a></h2>
<p>Previous appraoches focus on scaling-out by data partitioning, but most of applications do not have large amount of data. It is not necssary to store data in multiple machines.</p>
<p>They propose to scale-out by replication in a master-slave and asychronous fasion.</p>
<h2 id="problem-4"><a class="header" href="#problem-4">Problem</a></h2>
<p>To maintain a master-slave architecture on a DBMS system with high read scalability.</p>
<p>Main challenge: How to replicate data efficiently and ensure strong consistency?</p>
<h2 id="method-4"><a class="header" href="#method-4">Method</a></h2>
<p>(Quick read through)</p>
<p>Statement replication:</p>
<ol>
<li>Exceutes the SQL in the primary DB</li>
<li>Record the execution order of each statement</li>
<li>Replay the statements in the same ordre in backup DBs.</li>
</ol>
<h2 id="experiments-2"><a class="header" href="#experiments-2">Experiments</a></h2>
<p>(Not check)</p>
<h2 id="conclusion-3"><a class="header" href="#conclusion-3">Conclusion</a></h2>
<p>Pro</p>
<ul>
<li>Middleware approaches</li>
<li>Scales well for read-heavy workloads</li>
</ul>
<p>Con</p>
<ul>
<li>Not scale for write-heavy workloads since every write transactions must be executed in the primary DB once.</li>
<li>Only suitable for the cast that data can be stored in a single machine</li>
</ul>
<p>Compared to deterministic DBMSs</p>
<ul>
<li>No need to avoid ad-hoc queries.</li>
<li>No need to know read/write-set in advance.</li>
<li>However, deterministic DBMSs can deal with more general OLTP workloads.</li>
</ul>
<h2 id="questions-3"><a class="header" href="#questions-3">Questions</a></h2>
<ol>
<li>How to ensure low latency?
<ul>
<li>By using asychronous architecture to avoid 2PC.</li>
</ul>
</li>
<li>The master is still a bottleneck when using a master-slave architecture.
<ul>
<li>They assume most of transactions are read transactions, which can be routed to slave nodes.</li>
</ul>
</li>
<li>Why do the experiments show that Hihooi can still scale on write-heavy workloads?</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="deterministic-dbms"><a class="header" href="#deterministic-dbms">Deterministic DBMS</a></h1>
<ul>
<li><a href="deterministic_dbms/lu2020aria.html">VLDB'20 - Aria: A Fast and Practical Deterministic OLTP Database</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vldb14---an-evaluation-of-the-advantages-and-disadvantages-of-deterministic-database-systems"><a class="header" href="#vldb14---an-evaluation-of-the-advantages-and-disadvantages-of-deterministic-database-systems">VLDB'14 - An evaluation of the advantages and disadvantages of deterministic database systems</a></h1>
<ul>
<li>Authors: Kun Ren, Alexander Thomson, Daniel J. Abadi</li>
<li>Institute: Northestern Polytechnical University, Yale University</li>
<li>Published at VLDB'14</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/10.14778/2732951.2732955">https://dl.acm.org/doi/10.14778/2732951.2732955</a></li>
</ul>
<h2 id="goal"><a class="header" href="#goal">Goal</a></h2>
<p>To evaluate and compare deterministic DBMSs and non-deterministic DBMSs in different settings and workloads, in order to find out where to use deterministic DBMSs is the best.</p>
<h2 id="implementation-details"><a class="header" href="#implementation-details">Implementation Details</a></h2>
<h3 id="deterministic-dbmss"><a class="header" href="#deterministic-dbmss">Deterministic DBMSs</a></h3>
<ul>
<li>Use VLL protocol by default</li>
<li>1 thread for acquiring locks and 4 threads for processing transactions</li>
</ul>
<h3 id="non-deterministic-dbmss"><a class="header" href="#non-deterministic-dbmss">Non-deterministic DBMSs</a></h3>
<ul>
<li>5 threads are used for processing transactions
<ul>
<li>A thread can process multiple transactions at once, if most of transactions are waiting for network messages</li>
</ul>
</li>
<li>Lock-based protocol
<ul>
<li>Use wait-for graph to detect distributed deadlocks</li>
</ul>
</li>
<li>Uses two phase commit to ensure strong consistency</li>
</ul>
<h2 id="key-observations"><a class="header" href="#key-observations">Key Observations</a></h2>
<ul>
<li>Lock acquisition time for each transaction in deterministic DBMSs
<ul>
<li>30% for short transactions (1 read/write action for an item)</li>
<li>16% for long transactions</li>
</ul>
</li>
<li>VLL protocol is useful only when lock acquisition is a bottleneck.</li>
<li>Two phase commit makes a non-deterministic DBMS perform poorly when there are many distributed transactions.
<ul>
<li>About 30% in an extreme case.</li>
</ul>
</li>
<li>Distributed deadlocks makes a non-deterministic DBMS perform poorly when both the number of distributed transactions and the contention are high. (Figure 1, Figure 2)</li>
<li>How many nodes involve in a distributed transaction does not affect the performance difference between determinisitic and non-deterministic DBMSs. (Figure 3)</li>
<li>A non-deterministic DBMS can utilize CPU resource more when there is no distributed transaction with TPC-C because the overhead of handling distributed locking and deadlocks is eliminiated. (Figure 4)</li>
<li>It is often impossible for machines to get very far ahead of the slowest machine, since new transactions may have data dependencies on previous ones that access data on slow machines. (Figure 5 (a))
<ul>
<li>A non-deterministic DBMS can reorder transactions on demend to avoid this problem.</li>
</ul>
</li>
<li>The flexibility of non-deterministic DBMSs does not yield much benefit in a cluster with slow machines as we expected. (Figure 5 (a))
<ul>
<li>We can optimize non-determinitic DBMSs by aborting transactions (70% of local transactions).</li>
</ul>
</li>
<li>The performance cost of OLLP are independent of the performance cost of processing distributed transactions. (Figure 6)</li>
<li>For most real-world scenario, OLLP yields very few transaction restarts. (Figure 6)</li>
<li>A deterministic DBMS still scales better than a non-deterministic DBMS even on a high contention scenario because the non-deterministic DBMS needs to handle distributed deadlocks. (Figure 8)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aria-a-fast-and-practical-deterministic-oltp-database"><a class="header" href="#aria-a-fast-and-practical-deterministic-oltp-database">Aria: A Fast and Practical Deterministic OLTP Database</a></h1>
<ul>
<li>Authors: Yi Lu, Xiangyao Yu, Lei Cao, Samuel Madden</li>
<li>Institute: MIT</li>
<li>Published at VLDB'20</li>
<li>Paper Link: <a href="http://vldb.org/pvldb/vol13/p2047-lu.pdf">http://vldb.org/pvldb/vol13/p2047-lu.pdf</a>
<ul>
<li>Video: <a href="https://www.youtube.com/watch?v=DvgMjPPB134">https://www.youtube.com/watch?v=DvgMjPPB134</a></li>
</ul>
</li>
</ul>
<h2 id="background-2"><a class="header" href="#background-2">Background</a></h2>
<p>Deterministic DBMS show great potential for optimizations in transaction processing.</p>
<h2 id="motivation-5"><a class="header" href="#motivation-5">Motivation</a></h2>
<p>Currently, deterministic DBMSs all request the input transaction requests to provide their read-sets and write-sets. If not, they will need to execute the transactions once to determine their read-/write-sets.</p>
<h2 id="problem-5"><a class="header" href="#problem-5">Problem</a></h2>
<p>To design a concurrency control mechanism without knowing read-sets and write-sets while ensuring deterministic execution.</p>
<h2 id="method-5"><a class="header" href="#method-5">Method</a></h2>
<ul>
<li>Main Idea: Batch execution with barriers
<ul>
<li>Execution phase:
<ul>
<li>Executes one batch of transactions at a time.</li>
<li>Every transaction runs in parallel, reads from the same snapshot, and writes to its local buffer.</li>
<li>Updates to indices are also buffered, so there is no phantom due to index updates.</li>
</ul>
</li>
<li>Commit phase:
<ul>
<li>To commit a transaction, it must wait until all other transactions finish execution as well. (barrier)</li>
<li>If there is a WW, RW, or WR conflict with earlier transaction, aborts and reschedules the later transaction.</li>
</ul>
</li>
</ul>
</li>
<li>Optimization: Deterministic Reordering
<ul>
<li>Uses a relaxed check while deciding aborts:
<ul>
<li>Aborts a transaction only if:
<ul>
<li>It has WW conflict with an earlier transaction.</li>
<li>Or, it has at least one RW conflict and also at least one WR conflict with earlier transactions at the same time.
<ul>
<li>This rule prevents cycles in the dependency graph. (proved in Section 5.3)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Optimization: Fallback Phase
<ul>
<li>If too many transactions are aborted, add a fallback phase after the commit phase.</li>
<li>The fallback phase will execute the aborted transactions in the Calvin fashion.
<ul>
<li>The key is that we have known the read-sets and write-sets of the aborted transactions because the system has executed them once.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="experiments-3"><a class="header" href="#experiments-3">Experiments</a></h2>
<h3 id="my-expectation"><a class="header" href="#my-expectation">My Expectation</a></h3>
<ul>
<li>Aira works well in low contention workloads but poorly in high contention workloads.
<ul>
<li>It works actually ok in high contention workloads since it has fallback strategies.</li>
</ul>
</li>
</ul>
<h3 id="experiment-summary"><a class="header" href="#experiment-summary">Experiment Summary</a></h3>
<ul>
<li>8.2 YCSB
<ul>
<li>Aira works great since the keys of YCSB transactions are drawn from a uniform distribution.</li>
</ul>
</li>
<li>8.3 Scheduling Overhead
<ul>
<li>Aira has almost no scheduling overhead since the only overhead is to book-keeping writes in a reservation table.</li>
</ul>
</li>
<li>8.4 Effectiveness of Deterministic Reordering
<ul>
<li>The performance of Aira goes down as the workload becomes more skew, however, Aira still performs better than Calvin thanks to fallback phases.</li>
<li>Aira with deterministic reordering also shows its effectiveness compared to Aira without DR.</li>
</ul>
</li>
<li>8.5 TPC-C
<ul>
<li>Interestingly, this experiment shows how contention affects Aira significantly in a standard OLTP benchmarks.</li>
</ul>
</li>
<li>8.6 Distributed Transactions
<ul>
<li>Aira basically outperforms all baselines no matter how many distributed transactions are there.</li>
<li>However, note that the contention in the TPC-C setting is extremely low, which gives Aira a big advantage.</li>
</ul>
</li>
<li>8.7 Scalability
<ul>
<li>Aira scales well.</li>
</ul>
</li>
</ul>
<h2 id="conclusion-4"><a class="header" href="#conclusion-4">Conclusion</a></h2>
<p>Pros</p>
<ul>
<li>It won't need read-sets and write-sets for deterministic execution.</li>
<li>It performs much better than Calvin in low contention workloads.</li>
</ul>
<p>Cons</p>
<ul>
<li>Aborts many transactions in high contention workloads.</li>
<li>Barriers between batches leads to slow down the entire transaction execution when transaction lengths are imbalanced.</li>
</ul>
<h2 id="questions-4"><a class="header" href="#questions-4">Questions</a></h2>
<ul>
<li>Is that possible to use wound-wait or wait-die 2PL to achieve the same effect?
<ul>
<li>No, this may lead to nondeterministic execution since there is no barrier.</li>
</ul>
</li>
<li>The system aborts all the transactions that conflict with the earlier transactions in the same batch. So, does this mean that we better run this system in a low contention workloads?
<ul>
<li>Yes. See experiments in Section 8.5.</li>
</ul>
</li>
<li>How about long transactions that do not have conflicts with others? Does Aria suit the workloads with these transactions?
<ul>
<li>Figure 5 verifies this concern. If there are a few long transactions in a batch, it will greatly slow down the system.</li>
</ul>
</li>
<li>Why do they need barriers?
<ul>
<li>Consider the case that T1 does not run at all and T3 starts to commit in Example 1 of the paper. T3 may not find out T1 does not run since the system does not have T1's write-set. This makes the database state nondeterministic.</li>
<li>It also makes all transactions can run in parallel during the commit phase since all information that need to be checked are set during the execution phase.</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sigmod22---hybrid-deterministic-and-nondeterministic-execution-of-transactions-in-actor-systems"><a class="header" href="#sigmod22---hybrid-deterministic-and-nondeterministic-execution-of-transactions-in-actor-systems">SIGMOD'22 - Hybrid Deterministic and Nondeterministic Execution of Transactions in Actor Systems</a></h1>
<ul>
<li>Authors: Yijian Liu, Li Su, Vivek Shah, Yongluan Zhou, Marcos Antonio Vaz Salles</li>
<li>Institute: University of Copenhagen, Denmark</li>
<li>Published at SIGMOD'22</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/10.1145/3514221.3526172">https://dl.acm.org/doi/10.1145/3514221.3526172</a></li>
</ul>
<h2 id="background-3"><a class="header" href="#background-3">Background</a></h2>
<p>Now there are many applications using actor programming models:</p>
<ul>
<li>Games
<ul>
<li>Halo 4</li>
<li>League of Legends</li>
</ul>
</li>
<li>Telecommunication
<ul>
<li>Ericsson</li>
</ul>
</li>
<li>E-commerce
<ul>
<li>Paypal</li>
<li>Walmart</li>
</ul>
</li>
<li>IoT</li>
</ul>
<p>They have the demand of transactions such as:</p>
<ul>
<li>Purchasing equipment in games</li>
<li>E-commerce</li>
</ul>
<p>To fulfill transaction requirements for actors, Akka introduces <strong>transactors</strong>, which includes the ideas of:</p>
<ul>
<li>Two-phase Locking</li>
<li>Two-phase Commit</li>
<li>Early lock release</li>
</ul>
<h3 id="actor-oriented-databases-aodbs"><a class="header" href="#actor-oriented-databases-aodbs">Actor-oriented Databases (AODBs)</a></h3>
<ul>
<li>What is a AODB?
<ul>
<li>A database managed using the actor programming model</li>
<li>Each data actor manages an object or a series of objects</li>
<li>A transactional actor execute the logic and send requests to data actors
<ul>
<li>An actor might be both a transactional actor and a data actor</li>
</ul>
</li>
<li>A set of coordinators are reponsible for coordinating transactional actors</li>
</ul>
</li>
<li>Why?
<ul>
<li>The actor model is highly scalable
<ul>
<li>Actors use asynchronous message passing to avoid blocking and shared states</li>
<li>Since actors are not sharing states, it is easy to deploy actors on multiple machines</li>
</ul>
</li>
<li>In-memory =&gt; fast</li>
<li>Why not general-purpose DBMS?
<ul>
<li>Many backend systems using the actor programming model. AODBs are easier to integrate for them.</li>
</ul>
</li>
</ul>
</li>
<li>How does it work?
<ul>
<li>Game Example: Halo 4
<ul>
<li>Data Actors: players actors &amp; shop actor</li>
<li>Transactions: purchasing an item</li>
</ul>
</li>
<li>Financial Example: Bank Accounts
<ul>
<li>Data Actors: account actors</li>
<li>Transactions: transferring money</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="orleans"><a class="header" href="#orleans">Orleans</a></h3>
<p>Orleans is a framework for actor models. Key features:</p>
<ul>
<li>Virtual actors</li>
<li>Asynchronous message passing
<ul>
<li>But the order of messages is non-deterministic (may be out-of-order)</li>
</ul>
</li>
<li>Reentrancy
<ul>
<li>An actor is allowed to interleave multiple requests when some requests are waiting asynchronous operations.</li>
</ul>
</li>
</ul>
<h2 id="motivation-6"><a class="header" href="#motivation-6">Motivation</a></h2>
<p>However, the current design of transactions in actors makes all transactions become distributed transactions, even if the actors are at the same machine. This introduces significant amount of overhead to transactions.</p>
<p>This paper finds that some transactions of actor systems especially fit the idea of determinism, because all the parameters and participating actors are known in advance for those transactions. Determinism can greatly reduce the overhead of actor systems.</p>
<p>But, some other transactions still need to be executed non-deterministically, so how to make both execution work in a single system become a challenge.</p>
<h2 id="problem-6"><a class="header" href="#problem-6">Problem</a></h2>
<p>To design an architecture that can execute transactions in an actor system in both deterministic and non-deterministic modes.</p>
<p>A transaction is defined as a series of method invocation to multiple actors issued by an actor and requires <strong>conflict serializability</strong> and <strong>durability</strong>.</p>
<h3 id="assumptions-1"><a class="header" href="#assumptions-1">Assumptions</a></h3>
<p>Environments:</p>
<ul>
<li>Single machine</li>
<li>Actor models</li>
</ul>
<p>A transaction executed in the deterministic mode must provide:</p>
<ul>
<li>The main actor (who issue the transaction)</li>
<li>The first method to be invoked and corresponding inputs</li>
<li>The set of actors that this transaction is going to access</li>
</ul>
<h2 id="method-6"><a class="header" href="#method-6">Method</a></h2>
<h3 id="system-architecture"><a class="header" href="#system-architecture">System Architecture</a></h3>
<ul>
<li>Coordinator actors</li>
<li>Transactional actors </li>
<li>Loggers
<ul>
<li>Multiple loggers</li>
<li>Each logger has its own log file</li>
<li>Transactional actors sends its log to one of loggers decided by a simple hash function</li>
</ul>
</li>
</ul>
<h3 id="key-idea-to-ensure-serializability"><a class="header" href="#key-idea-to-ensure-serializability">Key Idea to ensure Serializability</a></h3>
<p>Perform a serializability check for all ACTs before they commit:</p>
<ul>
<li>For each ACT Ti, check if Ti depends on a batch Bi while a batch Bj with j &lt; i depends on Ti.
<ul>
<li>If the case exists, abort Ti.</li>
</ul>
</li>
</ul>
<p>This check ensures there is no cyclic dependency exist between PACTs and ACTs. Other possible violations to serializability have been prevented from the concurrency controls in PACTs and ACTs.</p>
<h2 id="experiments-4"><a class="header" href="#experiments-4">Experiments</a></h2>
<h3 id="base-settings"><a class="header" href="#base-settings">Base Settings</a></h3>
<ul>
<li>Environments
<ul>
<li>AWS EC2
<ul>
<li>4-core 3.0 GHz CPU</li>
<li>10.5 GB Memory</li>
<li>16GB SSD with 8K IOPS</li>
</ul>
</li>
</ul>
</li>
<li>Benchmarks
<ul>
<li>TPC-C
<ul>
<li>Only NewOrder transactions</li>
<li>Each warehouse is an actor, and the stock table is partitioned into multiple actors</li>
</ul>
</li>
<li>SmallBank
<ul>
<li>Add a new type: MultiTransfer transactions - transferring money from one account to multiple accounts</li>
<li>Each account is an actor</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="pact-vs-act"><a class="header" href="#pact-vs-act">PACT vs. ACT</a></h3>
<h4 id="impact-of-transaction-size"><a class="header" href="#impact-of-transaction-size">Impact of Transaction Size</a></h4>
<p>Varying transaction sizes with SmallBank's MultiTransfer transactions.</p>
<p>Throughput (Fig.12)</p>
<ul>
<li>Low transaction size -&gt; low contention
<ul>
<li>PACT needs more message exchanges -&gt; slower -&gt; lower throughput</li>
</ul>
</li>
<li>High transaction size -&gt; high contention
<ul>
<li>ACT aborts more transactions -&gt; lower throughput</li>
</ul>
</li>
<li>Logging
<ul>
<li>PACT can write logs in batches due to deterministic batching -&gt; more efficient</li>
</ul>
</li>
</ul>
<p>Latency (Fig.13)</p>
<ul>
<li>PACT's medium latency is almost the same as ACT's
<ul>
<li>Only when size = 64, PACT has higher medium latency due to the delay of batching</li>
</ul>
</li>
<li>ACT has higher 99th latency because of dynamic reordering of non-deterministic locking</li>
</ul>
<p>Conclusion</p>
<ul>
<li>PACT has more predictable latency and higher throughput in high contention workloads</li>
<li>ACT does better only in low contention workloads</li>
</ul>
<h4 id="impact-of-workload-skewness-fig14"><a class="header" href="#impact-of-workload-skewness-fig14">Impact of Workload Skewness (Fig.14)</a></h4>
<p>Deciding the keys/actors of transactions using Zipfian distribution with varying parameters. It also compares PACT &amp; ACT with Orleans' Txn. Orleans' Txn is basically ACT but with early lock release and timeout deadlock avoidance.</p>
<ul>
<li>Orleans' Txn loses in all kind of workloads even without deadlocks (explained in the next section)</li>
<li>ACT has lower throughput in higher skewness which makes sense.</li>
<li>PACT has higher throughput in higher skewness because batching become more efficient.</li>
</ul>
<h4 id="comparing-act-with-orleans-txn-fig15"><a class="header" href="#comparing-act-with-orleans-txn-fig15">Comparing ACT with Orleans' Txn (Fig.15)</a></h4>
<p>Comparing the latency of ACT with Orleans' Txn using a special type of transactions, each of that may do NO-OP to actors to test the overhead of maintaining a transaction in both systems.</p>
<ul>
<li>OrleansTxn has higher overhead in calling an actor and 2PC.</li>
</ul>
<h3 id="hybrid-execution-fig16"><a class="header" href="#hybrid-execution-fig16">Hybrid Execution (Fig.16)</a></h3>
<p>Running SmallBank with transaction size = 4</p>
<p>Throughput</p>
<ul>
<li>Hybrid execution yields lower throughput than the expectation. Reasons:
<ul>
<li>PACTs force ACTs to wait for batch processing</li>
<li>PACTs are blocked until the previous ACTs are committed</li>
<li>ACTs aborts more due to conflict with PACTs</li>
</ul>
</li>
<li>This situation becomes worse in high-skewed workloads</li>
</ul>
<p>Latency</p>
<ul>
<li>PACTs generally have higher latencies</li>
<li>As PACTs become less, PACTs run faster because of smaller batches, which has lower possibility to be blocked.</li>
<li>As ACTs become less, more long-latency ACTs are aborted due to higher possibility to conflict with PACTs.</li>
</ul>
<p>Aborts</p>
<ul>
<li>Most aborts come from read/write conflict of ACTs and serialiability check between PACTs and ACTs.</li>
</ul>
<h3 id="scalability-fig17"><a class="header" href="#scalability-fig17">Scalability (Fig.17)</a></h3>
<p>SmallBanks</p>
<ul>
<li>PACTs have better scalability in skewed workloads</li>
<li>All methods scale linearly</li>
</ul>
<p>TPC-C</p>
<ul>
<li>PACTs have better scalability in skewed workloads</li>
<li>All methods scale linearly</li>
<li>PACTs and ACTs have much lower throughput than NT due to inefficient logging methods.</li>
</ul>
<h2 id="conclusion-5"><a class="header" href="#conclusion-5">Conclusion</a></h2>
<h2 id="questions-5"><a class="header" href="#questions-5">Questions</a></h2>
<ul>
<li> What are 'transactors'?
<ul>
<li>Transactors: the actors that support transactional accesses</li>
</ul>
</li>
<li> Why does Orleans use 'virtual actors'?
<ul>
<li>They are just lightweight actors, which are only active when necessary</li>
</ul>
</li>
<li> Conflict Serializability
<ul>
<li>The most common way to define serializability for DBMSs, which is widely used in most lock-based DBMSs.</li>
</ul>
</li>
<li> How to ensure serializability while deterministic and non-deterministic txns co-exist?
<ul>
<li>See the example in Figure 8</li>
<li>See the key idea in the note above.</li>
</ul>
</li>
<li> Why did they use hybrid execution instead of non-deterministic only?
<ul>
<li>Deterministic execution has a few benefits:
<ul>
<li>No deadlock</li>
<li>Easy for batching</li>
</ul>
</li>
</ul>
</li>
<li> How often do deadlocks happen in hybrid execution? Looks like it is a big problem.
<ul>
<li>Interesting, Section 5.3.3 shows that only a small portion of transactions are aborted due to deadlocks.</li>
</ul>
</li>
<li> Do batch IDs of PACTs and txn IDs of ACTs come from the same counter?
<ul>
<li>It looks like it is. See Figure 8.</li>
</ul>
</li>
<li> Does this system have the cases of non-serializable transactions not due to deadlocks?</li>
<li> It seems like PACTs still need a commit protocol similar to 2PC to ensure the deterministic results. Then, what are the advantages PACTs have compared to ACTs?
<ul>
<li>No deadlock and batching</li>
</ul>
</li>
<li> Can PACTs commit without 2PC? Calvin does not need it, so this doesn't make sense that this system needs it.
<ul>
<li>It seems like PACTs still need 2PC because:
<ul>
<li>
<ol>
<li>PACTs runs in a master-slave manner</li>
</ol>
</li>
<li>
<ol start="2">
<li>Actors that execute ACTs should know the latest committed PACTs without communicating to coordinators</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li> If PACTs win because of batching, why not just batching ACTs?
<ul>
<li>PACTs also wins because it does not have deadlocks.</li>
<li>ACTs are not batched because it is hard to determine which transactions can be grouped by their access pattern.</li>
</ul>
</li>
<li> Key differences between Calvin and PACTs
<ul>
<li>Calvin replicates transactions to all partitions while PACTs are executed in a master-slave architecture</li>
<li>Calvin does not need 2PC while PACTs uses a 2PC-like architecture to ensure that coordinators and actors know the latest committed batches so that
<ul>
<li>
<ol>
<li>coordinators does not need to track dependencies</li>
</ol>
</li>
<li>
<ol start="2">
<li>actors can commit ACTs without communicating to coordinators</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dbms--ai"><a class="header" href="#dbms--ai">DBMS + AI</a></h1>
<ul>
<li><a href="dbms_with_ai/aken2017ottertune.html">SIGMOD'17 - Automatic Database Management System Tuning Through Large-scale Machine Learning</a></li>
<li><a href="dbms_with_ai/marcus2019dlquery.html">CIDR'19 - Towards a Hands-Free Query Optimizer through Deep Learning</a></li>
<li><a href="dbms_with_ai/zhou2020dbaisurvey.html">TKDE'20 - Database Meets AI: A Survey</a></li>
<li><a href="dbms_with_ai/hilprecht2022zeroshot.html">CIDR'22 - One Model to Rule them All: Towards Zero-Shot Learning for Databases</a></li>
</ul>
<h2 id="self-driving-dbmss"><a class="header" href="#self-driving-dbmss">Self-Driving DBMSs</a></h2>
<ul>
<li><a href="dbms_with_ai/lin2021mb2.html">SIGMOD'21 - MB2: Decomposed Behavior Modeling for Self-Driving Database Management Systems</a></li>
</ul>
<h2 id="multi-query-execution"><a class="header" href="#multi-query-execution">Multi-Query Execution</a></h2>
<ul>
<li><a href="dbms_with_ai/sioulas2021roulette.html">SIGMOD'21 - Scalable Multi-Query Execution using Reinforcement Learning</a></li>
</ul>
<h2 id="query-optimizer"><a class="header" href="#query-optimizer">Query Optimizer</a></h2>
<ul>
<li><a href="dbms_with_ai/yang2022balsa.html">SIGMOD'22 - Balsa: Learning a Query Optimizer Without Expert Demonstrations</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="automatic-database-management-system-tuning-through-large-scale-machine-learning"><a class="header" href="#automatic-database-management-system-tuning-through-large-scale-machine-learning">Automatic Database Management System Tuning Through Large-scale Machine Learning</a></h1>
<ul>
<li>Authors: Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, Bohan Zhang</li>
<li>Institute: CMU, Peking University</li>
<li>Published at SIGMOD'17</li>
<li>Paper Link: <a href="http://www.cs.cmu.edu/%7Epavlo/papers/p1009-van-aken.pdf">http://www.cs.cmu.edu/~pavlo/papers/p1009-van-aken.pdf</a></li>
</ul>
<h2 id="problem-7"><a class="header" href="#problem-7">Problem</a></h2>
<p>To tune the configurations of a DBMS using ML models.</p>
<h3 id="assumptions-2"><a class="header" href="#assumptions-2">Assumptions</a></h3>
<ul>
<li>The tuner must have administrative privileges to modify the DBMS's configurations.</li>
<li>The cost of restarting a DBMS is ignored.</li>
<li>The physical design is reasonable.
<ul>
<li>Proper indexes, materialized views, other database elements have been installed.</li>
</ul>
</li>
</ul>
<h2 id="method-7"><a class="header" href="#method-7">Method</a></h2>
<p><img src="dbms_with_ai/aken2017ottertune-figure3.png" alt="Overview" /></p>
<h3 id="workload-characterization"><a class="header" href="#workload-characterization">Workload Characterization</a></h3>
<p>OtterTune collects the <strong>internal</strong> metrics because those metrics directly relate to the knobs and more predictable when tuning knobs.</p>
<ul>
<li>the number of pages read/writes</li>
<li>query cache utilization</li>
<li>locking overhead</li>
</ul>
<h4 id="how-to-pick-up-useful-metrics"><a class="header" href="#how-to-pick-up-useful-metrics">How to Pick Up Useful Metrics</a></h4>
<p>Some metrics may redundant because</p>
<ul>
<li>they are the same but in different units (MB/KB...)</li>
<li>they are highly correlated</li>
</ul>
<p>Steps:</p>
<ol>
<li>Build a matrix \(X\) where \(X_{ij}\) represents the value of metric \(i\) on configuration set \(j\)</li>
<li>Performs Factor Analysis to reduce the dimension of \(X\) to \(U\) where \(U_{ij}\) represents the value of metric \(i\) on the \(j\)-th factor</li>
<li>Performs k-means clustering and pick up only the most representative metric in each cluster
<ul>
<li>\(K\) is determined by a heuristic algorithm without human intervention</li>
</ul>
</li>
</ol>
<p>Example Results:</p>
<p><img src="dbms_with_ai/aken2017ottertune-figure1.png" alt="Picking Useful Metrics" /></p>
<h3 id="knob-identification"><a class="header" href="#knob-identification">Knob Identification</a></h3>
<ul>
<li>Use LASSO to evaluate the impact of each knobs
<ul>
<li>\(X\): knobs</li>
<li>\(y\): metrics</li>
<li>The most common feature selection algorithm</li>
<li>Computationally efficient</li>
</ul>
</li>
<li>Includes polynomial features to test if there is dependency between two knobs
<ul>
<li>For example, product &quot;Buffer Pool Size&quot; and &quot;Log Buffer Size&quot; as a feature to see if LASSO pick up this feature</li>
</ul>
</li>
<li>Use incremental approach (gradually increase the number of selected knobs/features and check the effectiveness)</li>
</ul>
<p>Example Results:</p>
<p><img src="dbms_with_ai/aken2017ottertune-figure2.png" alt="LASSO" /></p>
<h3 id="automatic-tuner"><a class="header" href="#automatic-tuner">Automatic Tuner</a></h3>
<p>Steps</p>
<ol>
<li>Find the most similar workload in the past (workload mapping)
<ol>
<li>Build a matrix \(X_m\) for each metric \(m\) where \(X_{mij}\) represents the value of metric \(m\) when running the DBMS on workload \(i\) with configuration set \(j\)
<ul>
<li>The values must be normalized.</li>
</ul>
</li>
<li>Compute euclidean distance for the target workload \(i\) with other rows in the same matrix</li>
<li>Average the distance for each row/workload across matrixes as <strong>scores</strong></li>
<li>Choose the workload id with the lowest score as the most similar workload</li>
</ol>
</li>
<li>Use Gaussian Process (GP) to predict the best configuration set</li>
</ol>
<h2 id="conclusion-6"><a class="header" href="#conclusion-6">Conclusion</a></h2>
<p>Interesting insights</p>
<ul>
<li>Uses not only external metrics but also internal metrics for evaluating the performance of a configuration</li>
<li>The way of picking up the useful metrics</li>
</ul>
<h2 id="questions-6"><a class="header" href="#questions-6">Questions</a></h2>
<ul>
<li>How do they use the dependencies between knobs? Do those become features?
<ul>
<li>Not sure</li>
</ul>
</li>
<li>Do they use the variance given by Gaussian Process?
<ul>
<li>They use the variance as the confidence level</li>
</ul>
</li>
<li>Does OtterTune use any workload information such as queries or transactions for tuning?
<ul>
<li>No</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="towards-a-hands-free-query-optimizer-through-deep-learning"><a class="header" href="#towards-a-hands-free-query-optimizer-through-deep-learning">Towards a Hands-Free Query Optimizer through Deep Learning</a></h1>
<ul>
<li>Authors: Ryan Marcus, Olga Papaemmanouil</li>
<li>Institute: Brandeis University</li>
<li>Published at CIDR'19</li>
<li>Paper Link: <a href="http://cidrdb.org/cidr2019/papers/p96-marcus-cidr19.pdf">http://cidrdb.org/cidr2019/papers/p96-marcus-cidr19.pdf</a></li>
</ul>
<h2 id="background-4"><a class="header" href="#background-4">Background</a></h2>
<p>Query optimization is a popular and important research topic.</p>
<h2 id="motivation-7"><a class="header" href="#motivation-7">Motivation</a></h2>
<p>There are chances for deep reinforcement learning to help query optimization:</p>
<ul>
<li>Many optimization approaches are heuristics due to the complexity of the problem.</li>
<li>Deep RL can learn from mistakes.</li>
</ul>
<h2 id="problem-8"><a class="header" href="#problem-8">Problem</a></h2>
<p>To study if it is possible to use deep RL to generate a plan tree for a query.</p>
<h2 id="case-study-rejoin"><a class="header" href="#case-study-rejoin">Case Study: ReJOIN</a></h2>
<p>It models query planning as a deep RL problem. Each time planning for a query is an episode.</p>
<ul>
<li>State: relations (tables)
<ul>
<li>Not sure how exactly it is</li>
</ul>
</li>
<li>Action: which two relations to join</li>
<li>Reward: the estimated cost from the query cost estimator
<ul>
<li>Only gives the reward when the agent reaches the final state.</li>
</ul>
</li>
</ul>
<h2 id="challenges"><a class="header" href="#challenges">Challenges</a></h2>
<ul>
<li>Large Search Space Size
<ul>
<li>The search space is extremely large if we want to let the RL agent deal with all the operators</li>
</ul>
</li>
<li>Hard to provide reward
<ul>
<li>To efficiently train an agent, rewards need to be dense. However, if we choose query latency to be rewards, rewards would be sparse.</li>
<li>The estimated cost is also not a good indicator for rewards because the cost estimator needs to be tuned by humans.</li>
</ul>
</li>
<li>High evaluation overhead
<ul>
<li>It is hard for the agent to come out a good plan in the beginning. It may take much longer time to evaluate the plans.</li>
</ul>
</li>
</ul>
<h2 id="comments-1"><a class="header" href="#comments-1">Comments</a></h2>
<ul>
<li>Is it possible to solve the evaluation problem with curriculum learning? Like starting from a easy problem.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="database-meets-ai-a-survey"><a class="header" href="#database-meets-ai-a-survey">Database Meets AI: A Survey</a></h1>
<ul>
<li>Authors: Xuanhe Zhou, Chengliang Chai, Guoliang Li, JI SUN</li>
<li>Institute: Tsinghua Unversity, Beijing, China</li>
<li>Published at TKDE'20</li>
<li>Paper Link: <a href="https://ieeexplore.ieee.org/document/9094012">https://ieeexplore.ieee.org/document/9094012</a></li>
</ul>
<h2 id="learning-based-database-configuration"><a class="header" href="#learning-based-database-configuration">Learning-based Database Configuration</a></h2>
<h3 id="knob-tuning"><a class="header" href="#knob-tuning">Knob Tuning</a></h3>
<p>Problem: to find the best set of configurations for a DBMS.</p>
<h4 id="search-based-tuning"><a class="header" href="#search-based-tuning">Search-based Tuning</a></h4>
<p>Finding the best configurations by branching and bound.</p>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3127479.3128605">SoCC'17 - BestConfig: tapping the performance potential of systems via automatic configuration tuning</a>
<ul>
<li>Method
<ol>
<li>Divides the search space into smaller subspaces</li>
<li>Sampling from the subspaces and iteractively reduces the search space to find the best one</li>
</ol>
</li>
<li>Cons
<ul>
<li>Heuristic, no guarantee to find the best one</li>
<li>The search space is too large</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="traditional-ml-based-tuning"><a class="header" href="#traditional-ml-based-tuning">Traditional ML-based Tuning</a></h4>
<p>Finding the bets configurations using traditional ML-based methods.</p>
<ul>
<li><a href="https://www.cs.cmu.edu/%7Edvanaken/papers/ottertune-sigmod17.pdf">SIGMOD'17 - Automatic Database Management System Tuning ThroughLarge-scale Machine Learning</a>
<ul>
<li>Alias: OutterTune</li>
<li><a href="dbms_with_ai/./aken2017ottertune.html">Read Note</a></li>
</ul>
</li>
<li><a href="https://openproceedings.org/2019/conf/edbt/EDBT19_paper_226.pdf">EDBT'19 - SparkTune: tuning Spark SQL through query cost modeling</a></li>
<li>Cons
<ul>
<li>The optimal solution obtained in the current stage is not guaranteed to be optimal in other stages.</li>
<li>Requires a large number of high quality samples for training.</li>
<li>Cannot support too many knobs.</li>
</ul>
</li>
</ul>
<h4 id="reinforcement-learning-for-tunning"><a class="header" href="#reinforcement-learning-for-tunning">Reinforcement Learning for Tunning</a></h4>
<p>Uses a Reinforcement Learning (RL) agent to find the best configurations for a DBMS.</p>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3299869.3300085">SIGMOD'19 - An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning</a>
<ul>
<li>Alias: CDBTune</li>
<li>Method
<ul>
<li>The RL Modeling:
<ul>
<li>Environment: a cloud DBMS</li>
<li>State: the internal metrics of the DBMS (similar to OutterTune)</li>
<li>Action: the values for increasing or decreasing configurations (knobs)</li>
<li>Reward: the difference of DBMS's performance</li>
<li>Agent Model: Deep Deterministic Policy Gradient (DDPG)</li>
</ul>
</li>
</ul>
</li>
<li>Pros
<ul>
<li>Does not need high-quality training data</li>
</ul>
</li>
<li>Cons
<ul>
<li>without considering workload features</li>
</ul>
</li>
</ul>
</li>
<li><a href="https://www.vldb.org/pvldb/vol12/p2118-li.pdf">VLDB'19 - QTune: A Query-Aware Database Tuning System with DeepReinforcement Learning</a>
<ul>
<li>Alias: QTune</li>
<li>Method
<ul>
<li>Basically same with CDBTune but considers workloads.</li>
<li>Uses Double-state Deep Reinforcement Learning (DS-DRL)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>(Reading...)</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mb2-decomposed-behavior-modeling-for-self-driving-database-management-systems"><a class="header" href="#mb2-decomposed-behavior-modeling-for-self-driving-database-management-systems">MB2: Decomposed Behavior Modeling for Self-Driving Database Management Systems</a></h1>
<ul>
<li>Authors: Lin Ma, William Zhang, Jie Jiao, Wuwen Wang, Matthew Butrovich, Wan Shen Lim, Prashanth Menon, Andrew Pavlo</li>
<li>Institute: Carnegie Mellon University</li>
<li>Published at SIGMOD'21</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/10.1145/3448016.3457276">https://dl.acm.org/doi/10.1145/3448016.3457276</a></li>
</ul>
<h2 id="background-creating-a-self-driving-dbms"><a class="header" href="#background-creating-a-self-driving-dbms">Background: Creating a Self-driving DBMS</a></h2>
<p>[51] proposes three main steps to build a self-driving DBMS:</p>
<ol>
<li>Workload forecasting
<ul>
<li>Predicting the future workloads</li>
</ul>
</li>
<li>Bahavior modeling
<ul>
<li>Predicting the runtime behavior relative to the target objective (latency, throughput) given the predicted workloads</li>
</ul>
</li>
<li>Decision making/planning
<ul>
<li>Selecting the actions to improve the objective</li>
</ul>
</li>
</ol>
<h2 id="motivation-behavior-modeling"><a class="header" href="#motivation-behavior-modeling">Motivation: Behavior Modeling</a></h2>
<h3 id="problem-formulation"><a class="header" href="#problem-formulation">Problem Formulation</a></h3>
<p>Input:</p>
<ul>
<li>The workload</li>
<li>The system state</li>
<li>An action</li>
</ul>
<p>Output:</p>
<ul>
<li>How long the action takes</li>
<li>How much resource the action consumes</li>
<li>How applying the action impacts the system performance</li>
<li>How the action impacts the system once it is deployed</li>
</ul>
<h3 id="current-approaches"><a class="header" href="#current-approaches">Current Approaches</a></h3>
<ul>
<li>White-box analytical methods [42, 45, 74]
<ul>
<li>Use a human-devised formula</li>
<li>Built for specific DBMSs
<ul>
<li>Different systems may use different formula</li>
</ul>
</li>
<li>Con: difficult to migrate to a new DBMS</li>
</ul>
</li>
<li>ML methods
<ul>
<li>Use a ML model</li>
<li>Pro: more scalable and adaptable</li>
<li>Cons
<ul>
<li>Only designed for query cost estimation
<ul>
<li>E.g., uses query plan to predict latency</li>
</ul>
</li>
<li>Current models do not consider OLTP workloads</li>
<li>Need accurate information for workloads (hard for predicting future workloads)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p>The impact of creating a secondary index on the TPC-C workloads with 4 and 8 threads.</p>
<p><img src="dbms_with_ai/lin2021mb2-fg1.png" alt="Figure 1" /></p>
<p>The behavior model must predict this impact with the given action.</p>
<h3 id="challenges-1"><a class="header" href="#challenges-1">Challenges</a></h3>
<ul>
<li>High dimensionality
<ul>
<li>Building a model to predict the performance of a DBMS must need a high-dimensional features, which impact the performance of the model.</li>
</ul>
</li>
<li>Concurrent operations
<ul>
<li>Many concurrent transactions must affect the predicted result.</li>
</ul>
</li>
<li>Training data collection v.s. Generalizability
<ul>
<li>To improve generalizability of a model, the system must collect more training data.</li>
<li>However, collecting training data also requires large amount of effort.</li>
</ul>
</li>
</ul>
<h2 id="related-work"><a class="header" href="#related-work">Related Work</a></h2>
<ul>
<li>ML Models
<ul>
<li>Mostly based on query plans</li>
<li>Needs to retrain the entire models if anything changes in the DBMS</li>
<li>Poor generalizability between workloads.</li>
<li>Focus on OLAP workloads</li>
</ul>
</li>
<li>Analytical Models
<ul>
<li>Mostly designed for a special purpose
<ul>
<li>for resource bottleneck</li>
<li>for cardinality estimation</li>
<li>for index defragmentation suggestions</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="goal-1"><a class="header" href="#goal-1">Goal</a></h2>
<p>To design a general behavior modeling method.</p>
<h3 id="assumptions-3"><a class="header" href="#assumptions-3">Assumptions</a></h3>
<ul>
<li>Workloads are predictable</li>
<li>In-memory DBMS with MVCC</li>
<li>Supporting both OLTP and OLAP workloads</li>
<li>Supporting capturing lock contention</li>
<li>Does not consider aborts due to data conflicts</li>
</ul>
<h2 id="method-8"><a class="header" href="#method-8">Method</a></h2>
<h3 id="main-idea-1"><a class="header" href="#main-idea-1">Main Idea</a></h3>
<p>Decomposing the DBMS into independent opearting units (OU), each of which represents a step to complete a specific task.</p>
<p>Then, MB2 creates a OU-runner and a OU-model for each OU:</p>
<ul>
<li>OU-runner: a runner to search input space, collect training data and train a model.</li>
<li>OU-model: a ML model for the OU.</li>
</ul>
<h3 id="flow"><a class="header" href="#flow">Flow</a></h3>
<p><img src="dbms_with_ai/lin2021mb2-fg3.png" alt="Figure 3" /></p>
<ol>
<li>Given
<ul>
<li>a forecasted workloads</li>
<li>a candidate action</li>
</ul>
</li>
<li>Translating the action to features for OU</li>
<li>Making all OU-models to predict the results</li>
<li>Using an interference model to adjust the results</li>
<li>Merging the results to the predicted system performance</li>
</ol>
<h3 id="operating-units-ou"><a class="header" href="#operating-units-ou">Operating Units (OU)</a></h3>
<p>Key properties of an OUs:</p>
<ul>
<li>Independent: the runtime behavior of an OU is independent of other OUs.
<ul>
<li>E.g., Changing join hash table size does not affect WAL.</li>
</ul>
</li>
<li>Low dimensional: an model for an OU does not need many features to accurately predict the performance.
<ul>
<li>Insight: # of features = 10 is good</li>
<li>Divide an OU to multiple OUs if it needs more features.</li>
</ul>
</li>
<li>Comprehensive: OUs must cover all operations in a DBMS.</li>
</ul>
<p>OU Examples:</p>
<p><img src="dbms_with_ai/lin2021mb2-tb1.png" alt="Table 1" /></p>
<p>OU Types:</p>
<ul>
<li>Singular: focus on work and resource consumption for a single invocation.</li>
<li>Batch: focus on a batch of work across OUs.</li>
<li>Contending: focus on the work that may contend with other threads</li>
</ul>
<h3 id="ou-models"><a class="header" href="#ou-models">OU-Models</a></h3>
<h4 id="input-features"><a class="header" href="#input-features">Input Features</a></h4>
<ul>
<li>Singular
<ul>
<li>number of input tuples</li>
<li>number of columns of input tuples</li>
<li>average input tuple size</li>
<li>estimated key cardinality (e.g., sorting, joins)</li>
<li>payload size (e.g., hash table entry size for hash join)</li>
<li>number of loops (for index nested loop joins)</li>
<li>is interpreter or JIT-compiled</li>
</ul>
</li>
<li>Batch
<ul>
<li>total number of bytes</li>
<li>total number of log buffers</li>
<li>log flush interval</li>
</ul>
</li>
<li>Contending
<ul>
<li>number of tuples</li>
<li>number of keys</li>
<li>size of keys</li>
<li>estimated cardinality of the keys</li>
<li>number of parallel threads</li>
</ul>
</li>
</ul>
<p>In addition to the above features, it also append tuneable configurations (knobs) for the OU to the features as inputs.</p>
<h4 id="output-labels"><a class="header" href="#output-labels">Output Labels</a></h4>
<ul>
<li>elapsed time</li>
<li>CPU time</li>
<li>CPU cycles</li>
<li>CPU instructions</li>
<li>CPU cache references</li>
<li>CPU cache misses</li>
<li>disk block reads</li>
<li>disk block writes</li>
<li>memory consumption</li>
</ul>
<p>Note that the labels are the same for all OUs, so that MB2 can combine them together easier.</p>
<h4 id="problems-of-collecting-data-with-olap-queries"><a class="header" href="#problems-of-collecting-data-with-olap-queries">Problems of collecting data with OLAP queries</a></h4>
<p>OLAP queries usually takes much more time to process, which lead to high overhead of collecting training data for them.</p>
<p>In order to overcome this, they normalize the output labels by the number of tuples so that we can train the model with queries that access less tuples.</p>
<p>The value is normalized according to the following observation:</p>
<ul>
<li>They observed that the value of output labels is usually a complexity related to n (number of tuples) times a constant.</li>
<li>So, they normalize the values by dividing the complexity.</li>
<li>A special case: memory consumption for building hash tables.</li>
</ul>
<h3 id="the-interference-model"><a class="header" href="#the-interference-model">The Interference Model</a></h3>
<p>To adjust the outputs of OU-models due to resource competition between OUs.</p>
<p><img src="dbms_with_ai/lin2021mb2-fg4.png" alt="Figure 4" /></p>
<h4 id="key-ideas"><a class="header" href="#key-ideas">Key Ideas</a></h4>
<ul>
<li>Normalized the inputs by the elapsed time.</li>
<li>Predicting the ratio of the actual values and OU-model's predicted values.</li>
</ul>
<p>The key ideas is based on an observation:</p>
<blockquote>
<p>We observe that under the same concurrent environment,
OUs with similar per-time-unit OU-model estimation (part of the
interference models inputs) experience similar impacts and have
similar output ratios regardless of the absolute elapsed time.</p>
</blockquote>
<h4 id="inputs"><a class="header" href="#inputs">Inputs</a></h4>
<ul>
<li>A OU-model's output labels</li>
<li>Summary statistics of the OUs forecasted to run in the same time interval (e.g., 1 minute)
<ul>
<li>Sum</li>
<li>Variance</li>
</ul>
</li>
</ul>
<p>Normalized by dividing inputs by the target OU-model's estimated elapsed time.</p>
<h4 id="outputs"><a class="header" href="#outputs">Outputs</a></h4>
<p>Same output labels with the input OU-model, but the values are the ratio between actual metrics and the original predicted metrics. The ratios usually &gt;= 1 since an OU runs faster by itself.</p>
<h3 id="training-data-collection-and-training"><a class="header" href="#training-data-collection-and-training">Training Data Collection and Training</a></h3>
<p>Assumption: off-line</p>
<h4 id="components"><a class="header" href="#components">Components</a></h4>
<ul>
<li>OU Translator: translating queries and actions to OUs' inputs.</li>
<li>Resource Tracker: tracking the elapsed time and resource consumptions for each OU.
<ul>
<li>Use user- and kernel-level functions to track.</li>
</ul>
</li>
<li>OU-Runner: a microbenchmark to generate data for all possible inputs for each OU.
<ul>
<li>Inputs are generated in fixed-length and exponential step sizes.</li>
<li>MB2 will normalize the output labels, which greatly reduces the number of training data that need to be collected.</li>
<li>Can be executed concurrently with other OU-runners for training the inference model.
<ul>
<li>Parameters for using concurrent runners:
<ul>
<li>Which subsets of queries to execute</li>
<li>The number of concurrent threads in the DBMS</li>
<li>The workload submission rate.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="handling-the-inference-of-tracking-data-from-other-ous"><a class="header" href="#handling-the-inference-of-tracking-data-from-other-ous">Handling the inference of tracking data from other OUs</a></h4>
<p>Challenges when collecting training data:</p>
<ul>
<li>multiple threads produce metrics in the same time and thus requires coordination</li>
<li>too many resource tracker may incur a noticeable cost.</li>
</ul>
<p>These issues are addressed by:</p>
<ul>
<li>Makes each thread tracks their own metrics and uses a dedicated aggregator to gather these data and store together.</li>
<li>Turning off other OUs' resource tracker during collecting data.</li>
</ul>
<h4 id="challenges-for-collecting-data-for-oltp-queries"><a class="header" href="#challenges-for-collecting-data-for-oltp-queries">Challenges for collecting data for OLTP queries</a></h4>
<ul>
<li>High variance due to hardware (e.g., CPU scaling) and background noise (e.g., kernel tasks)
<ul>
<li>Solution: execute OU-runner for each OU with sufficient repetitions (10 times) and applies robust statistics.
<ul>
<li>Uses 20% trimmed mean statistics</li>
</ul>
</li>
</ul>
</li>
<li>A DBMS may execute OLTP queries as prepared statements
<ul>
<li>Solution: execute each query 5 times for warm-up</li>
</ul>
</li>
<li>Other details:
<ul>
<li>Starts a new transaction for each execution to avoid data residing in CPU caches.</li>
<li>If a query modifies database state, revert the query by rolling back the transaction.</li>
</ul>
</li>
</ul>
<p>Labels are insensitive to the trimmed mean percentage and number of warm-ups.</p>
<h4 id="models-selection"><a class="header" href="#models-selection">Models Selection</a></h4>
<p>MB2 selects and trains models for each OU and the inference model in the following steps:</p>
<ol>
<li>Split the training data to train/validation set (8:2)</li>
<li>Train the following models and perform cross-validation
<ul>
<li>Linear regression</li>
<li>Huber regression</li>
<li>SVM</li>
<li>Kernel regression</li>
<li>Random forest</li>
<li>Gradient boosting machine</li>
<li>Deep neural network</li>
</ul>
</li>
<li>Select the one with the highest validation score</li>
<li>Train the selected model with all available training data</li>
</ol>
<h4 id="for-system-updates"><a class="header" href="#for-system-updates">For system updates</a></h4>
<p>MB2 only needs to retrain the OU-models for the affected OUs.</p>
<h2 id="experiments-5"><a class="header" href="#experiments-5">Experiments</a></h2>
<h3 id="environment"><a class="header" href="#environment">Environment</a></h3>
<ul>
<li>Hardware:
<ul>
<li>2 x Intel Xeon E5-2630v4 CPUs (20 Cores)</li>
<li>128 GB RAM</li>
<li>Intel Optane DC P4800X SSD (NVMe)</li>
</ul>
</li>
<li>OS: Ubuntu 18.04 LTS</li>
<li>DBMS: NoisePage</li>
<li>ML Framework: scikit-learn
<ul>
<li>All parameters remain default:
<ul>
<li>Random forest: 50 estimators</li>
<li>Deep NN: 2 layers with 25 neurons</li>
<li>Gradient boosting machine: 20 depth &amp; 1000 leaves</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h3>
<p>OLTP-Bench [13]:</p>
<ul>
<li>SmallBank: OLTP, 3 tables, 5 tx types
<ul>
<li>Models customers interacting with a bank branch</li>
</ul>
</li>
<li>TATP: OLTP, 4 tables, 7 tx types
<ul>
<li>Models cell phone registration service</li>
</ul>
</li>
<li>TPC-C: OLTP, 9 tables, 5 tx types
<ul>
<li>Models warehouses fulfilling orders</li>
</ul>
</li>
<li>TPC-H: OLAP, 8 tables, long-length queries
<ul>
<li>Models business analytics workload</li>
</ul>
</li>
</ul>
<h3 id="evaluation-metics"><a class="header" href="#evaluation-metics">Evaluation Metics</a></h3>
<ul>
<li>Relative Error: \(\frac{|Actual - Predict|}{Actual}\) for OLAP workloads</li>
<li>Average Absolute Error: \(|Actual - Predict|\) per OLTP query template</li>
</ul>
<h3 id="training-and-model-cost"><a class="header" href="#training-and-model-cost">Training and Model Cost</a></h3>
<p><img src="dbms_with_ai/lin2021mb2-tb2.png" alt="Table 2" /></p>
<p>Key results:</p>
<ul>
<li>1M unique data points for 19 OUs</li>
<li>Average Inference time
<ul>
<li>OU translator for a query: 10 s</li>
<li>OU model for a query: 0.5 ms</li>
</ul>
</li>
<li>Average resource tracker invocation time: 20 s</li>
</ul>
<h3 id="ou-model-accuracy"><a class="header" href="#ou-model-accuracy">OU-Model Accuracy</a></h3>
<p><img src="dbms_with_ai/lin2021mb2-fg5.png" alt="Figure 5" /></p>
<p>Key insights:</p>
<ul>
<li>More than 80% of the OU-models have an average error &lt; 20%</li>
<li>Transaction OU-models and probing an aggregation hash table have higher relative error because most cases have short elapsed time (&lt; 10 s), which leads to high variance.</li>
<li>Random forest and gradient boosting machine perform best</li>
<li>Deep NN have higher error because most of them overfit on low dimension data.</li>
<li>Huber regression is also effective for simple OUs and cheaper to train.</li>
</ul>
<p><img src="dbms_with_ai/lin2021mb2-fg6.png" alt="Figure 6" /></p>
<p>Key insights:</p>
<ul>
<li>Most labels have an average error &lt; 20%</li>
<li>Predicting cache miss is challenging because it depends on the content in the cache</li>
<li>Normalization is effective</li>
</ul>
<h3 id="generalizability-on-query-runtime-prediction"><a class="header" href="#generalizability-on-query-runtime-prediction">Generalizability on Query Runtime Prediction</a></h3>
<p>Baseline: QPPNet [26, 40]</p>
<ul>
<li>A tree-structured neural network</li>
<li>The state-of-the-art on predicting query runtime</li>
<li>Generalizability is good</li>
<li>Disk-based</li>
</ul>
<p>Training Method</p>
<ul>
<li>For OLAP, training on TPC-H 1G data set and testing on all other OLAP workloads.</li>
<li>For OLTP, training on TPC-C data set and testing on all other OLTP workloads.</li>
</ul>
<p><img src="dbms_with_ai/lin2021mb2-fg7.png" alt="Figure 7" /></p>
<p>Key insights:</p>
<ul>
<li>OLAP
<ul>
<li>QPPNet achieves competitive performance on the workload it trains on, but it has higher errors on other workloads.</li>
<li>MB2 achieve better and stable performance across all workloads because the fine-grained OUs design.</li>
<li>Output normalization technique helps.</li>
</ul>
</li>
<li>OLTP
<ul>
<li>MB2 has higher error on TPC-C, but it generalizes better to other workloads.</li>
<li>Output normalization does not help too much.</li>
</ul>
</li>
</ul>
<h3 id="the-interference-model-1"><a class="header" href="#the-interference-model-1">The Interference Model</a></h3>
<p>Settings:</p>
<ul>
<li>The interference model uses deep NN (which performs best).</li>
<li>Executes the queries in both single-thread and concurrent environments and compare the true adjustment factors against the predicted adjustment factors</li>
</ul>
<p><img src="dbms_with_ai/lin2021mb2-fg8.png" alt="Figure 8" /></p>
<p>Key insights:</p>
<ul>
<li>The interference model has less than 20% error in all cases.</li>
<li>Small data set size results in higher variance in the interference, so the model has higher error.</li>
</ul>
<h3 id="model-adaptation-and-robustness"><a class="header" href="#model-adaptation-and-robustness">Model Adaptation and Robustness</a></h3>
<h4 id="system-updates"><a class="header" href="#system-updates">System Updates</a></h4>
<p>Settings:</p>
<ul>
<li>Simulate system updates of improving join hash table algorithm adding sleep time:
<ul>
<li>No sleep</li>
<li>Sleep 1 us very 1000 insertions</li>
<li>Sleep 1 us very 100 insertions</li>
</ul>
</li>
<li>MB2 retrains the OU-models for hash join
<ul>
<li>Takes 23 minutes (24x faster than retraining all OU-models)</li>
</ul>
</li>
</ul>
<p>They seems to put the wrong figure for this experiment. (Figure 9a)</p>
<h4 id="noisy-cardinality"><a class="header" href="#noisy-cardinality">Noisy Cardinality</a></h4>
<p>Settings:</p>
<ul>
<li>Add Gaussian white noise (mean = 0, variance = 30%) on cardinality estimation, which is an important input features for OU-models</li>
</ul>
<p><img src="dbms_with_ai/lin2021mb2-fg9b.png" alt="Figure 9b" /></p>
<p>Key insights:</p>
<ul>
<li>Has almost no accuracy loss (&lt; 2%)</li>
</ul>
<h3 id="hardware-adaptability-by-adding-hardware-context-as-features"><a class="header" href="#hardware-adaptability-by-adding-hardware-context-as-features">Hardware Adaptability by Adding Hardware Context as Features</a></h3>
<p>Settings</p>
<ul>
<li>Adds CPU frequency as one of input features for OU-models</li>
<li>Tests OU-models trained using different CPU frequency (1.2 ~ 3.1 GHz)</li>
</ul>
<p><img src="dbms_with_ai/lin2021mb2-fg10.png" alt="Figure 10" /></p>
<p>Key insights:</p>
<ul>
<li>Extending the OU-model with hardware context improves the prediction in most cases</li>
<li>A special case where it performs notably worse is for the TPC-C workload under 2.0 GHz CPU
<ul>
<li>Because the models generally over-predict the runtime of the TPC-C queries.</li>
</ul>
</li>
</ul>
<h3 id="end-to-end-self-driving-execution"><a class="header" href="#end-to-end-self-driving-execution">End-to-End Self-Driving Execution</a></h3>
<p>Settings:</p>
<ul>
<li>Assumes
<ul>
<li>a forecaster that forecasts the average query arrival rate per query type in the next 10 seconds.</li>
<li>a decision maker that uses the estimated information to decide how to adjust the system.</li>
</ul>
</li>
<li>Workloads: daily transactional-analytical workload cycle
<ul>
<li>TPC-C: 20 warehouses, 50000 customers per district</li>
<li>TPC-H: 1GB</li>
<li>10 concurrent threads</li>
</ul>
</li>
<li>Initial configurations that need to be updated
<ul>
<li>Interpretive mode (JIT works better)</li>
<li>No secondary index for customer tables</li>
</ul>
</li>
</ul>
<p>Goal: to see whether MB2 can accurately estimate the latency with given action plans.</p>
<p><img src="dbms_with_ai/lin2021mb2-fg11a.png" alt="Figure 11a" /></p>
<p>With workload changes and the decisions (changing execution mode and building an index), MB2 accurately predicts the latency.</p>
<p><img src="dbms_with_ai/lin2021mb2-fg11c.png" alt="Figure 11c" /></p>
<p>Even if we change the actions (building index with fewer threads), MB2 still manages to predict the latency accurately.</p>
<p><img src="dbms_with_ai/lin2021mb2-fg11b.png" alt="Figure 11b" /></p>
<p>MB2 can also accurately predicts CPU utilization for each query.</p>
<h2 id="conclusion-7"><a class="header" href="#conclusion-7">Conclusion</a></h2>
<ul>
<li>Provides many useful insights and techniques for latency estimation.</li>
<li>Solid experiments</li>
<li>Due to the assumption of in-memory DBMS and MVCC, there is no discussion on modeling behaviors for disk I/Os and lock contentions.</li>
</ul>
<h2 id="questions-7"><a class="header" href="#questions-7">Questions</a></h2>
<ul>
<li>Why does the paper emphasize &quot;To orchestrate data collection across all OUs and to simulate concurrent environments, MB2 uses concurrent runners to execute end-to-end workloads (e.g., benchmarks, query traces) with multiple threads.&quot;? What does &quot;concurrent runners&quot; mean?</li>
<li>What is &quot;robust statistics&quot;?</li>
<li>Is it possible to predict the latency of a query without specify what action to perform for MB2?</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scalable-multi-query-execution-using-reinforcement-learning"><a class="header" href="#scalable-multi-query-execution-using-reinforcement-learning">Scalable Multi-Query Execution using Reinforcement Learning</a></h1>
<ul>
<li>Authors: Panagiotis Sioulas, Anastasia Ailamaki</li>
<li>Institute: EPFL</li>
<li>Published at SIGMOD'21</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/10.1145/3448016.3452799">https://dl.acm.org/doi/10.1145/3448016.3452799</a></li>
</ul>
<h2 id="background-5"><a class="header" href="#background-5">Background</a></h2>
<h3 id="vectorized-execution"><a class="header" href="#vectorized-execution">Vectorized Execution</a></h3>
<p>Vectorized execution  SIMD  query execution  SIMD  instruction  Vectorized execution  SIMD  query execution algorithm  vector vector  SIMD  query execution</p>
<p> vectorized execution </p>
<ul>
<li>Scan with filtering</li>
<li>Hash Table Probing</li>
<li>Histogram Building</li>
</ul>
<p>Reference: Andy Pavlo  <a href="https://15721.courses.cs.cmu.edu/spring2020/schedule.html">Vectorized Execution </a></p>
<h3 id="work-sharing"><a class="header" href="#work-sharing">Work-Sharing</a></h3>
<ul>
<li>Global Query Plan: a shared plan for multiple queries</li>
<li>Online sharing:  query query plan  query 
<ul>
<li> query plan  query plan  query plan  sub-plan query  query plan  global query plan online </li>
<li><a href="https://www.pdl.cmu.edu/PDL-FTP/Database/qpipe.pdf">SIGMOD'05 - QPipe</a>
<ul>
<li> work-sharing </li>
<li> reuse  query result  intermediate result</li>
<li> range </li>
</ul>
</li>
<li><a href="https://faculty.ucmerced.edu/frusu/Papers/Contribution/2010-sigmod-datapath.pdf">SIGMOD'10 - DataPath</a>
<ul>
<li> query  query  plan tree  global plan tree reuse common  sub-plan </li>
</ul>
</li>
<li><a href="https://dslab.epfl.ch/pubs/cjoin.pdf">VLDB'09 - CJOIN</a>
<ul>
<li> operator reordering selectivity </li>
</ul>
</li>
<li><a href="https://dsf.berkeley.edu/papers/sigmod02-cacq.pdf">SIGMOD'02 - CACQ</a></li>
</ul>
</li>
<li>Offline sharing:  query batch  cost 
<ul>
<li> solution space  query  complexity  scalability </li>
<li>Multi-query Optimization (MQO)
<ul>
<li> work </li>
<li> query  operator  query plan</li>
<li> bounding case </li>
</ul>
</li>
<li><a href="http://www.vldb.org/pvldb/vol7/p429-giannikis.pdf">VLDB'14 - Shared-workload Optimizers (SWO)</a>
<ul>
<li> MQO  batch of queries  input workload  query </li>
</ul>
</li>
</ul>
</li>
<li> SWO scalability issue
<ul>
<li>SharedDB</li>
<li>MQJoin</li>
</ul>
</li>
</ul>
<h3 id="adaptive-query-processing"><a class="header" href="#adaptive-query-processing">Adaptive Query Processing</a></h3>
<p> query execution  query plan</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=jveohy_qhHU">Symmetric Hash-join</a>
<ul>
<li> hash join  join table  (join key -&gt; record id)  hash table join table  record  hash table  match</li>
<li>Symmetric Hash-join  join table  hash table streamming query engine table  hash table hash table</li>
<li> hash table DBMS  stream process</li>
</ul>
</li>
<li><a href="https://dsf.berkeley.edu/cs286/papers/eddies-sigmod2000.pdf">SIGMOD'00 - Eddies</a>:  operator  input  output  reorder operator
<ul>
<li>Eddies  query plan  operator  ( hash join join order ) eddies  routing algorithm  tuple  join</li>
<li>Eddies  algorithm  tuple  operator ( join) operator  input  output  operator </li>
</ul>
</li>
<li><a href="https://dsf.berkeley.edu/papers/icde03-stems.pdf">ICDE'03 - State Modules (STeMs)</a>
<ul>
<li> hash table</li>
<li> SHJ 3-way  SHJ  join  hash table intermediate result  SHJ  join  hash table state modules  eddies  intermediate results  table base table  hash table Eddies  join  bash table</li>
</ul>
</li>
</ul>
<h3 id="reinforcement-learning"><a class="header" href="#reinforcement-learning">Reinforcement Learning</a></h3>
<p> Q-Learning  reorder operator</p>
<h3 id="learned-cardinality-estimation"><a class="header" href="#learned-cardinality-estimation">Learned Cardinality Estimation</a></h3>
<p> Learned Cardinality Estimation  cardinality</p>
<h2 id="motivation-8"><a class="header" href="#motivation-8">Motivation</a></h2>
<h2 id="problem-9"><a class="header" href="#problem-9">Problem</a></h2>
<h3 id="assumtions"><a class="header" href="#assumtions">Assumtions</a></h3>
<ul>
<li>OLAP Workloads
<ul>
<li>Almost no update to the database</li>
<li>Queries intend to summary the statistics of the database</li>
</ul>
</li>
<li> select-project-join (SPJ) 
<ul>
<li> SPJ  query plan  query plan </li>
</ul>
</li>
</ul>
<h2 id="method-9"><a class="header" href="#method-9">Method</a></h2>
<h3 id="main-contribution"><a class="header" href="#main-contribution">Main Contribution</a></h3>
<ul>
<li> RL-based  tuple router (eddy) online work sharing  global query plan</li>
</ul>
<h3 id=""><a class="header" href="#"></a></h3>
<ul>
<li>Episodes
<ul>
<li> episode  table  vector (vector size = 1024 tuples)eddy  global query plan executor  worker thread </li>
</ul>
</li>
</ul>
<h3 id="architecture"><a class="header" href="#architecture">Architecture</a></h3>
<ul>
<li>Main DBMS
<ul>
<li> query  plan tree</li>
<li> plan tree  SPJ  sub-plan  RouLette  sub-plan  RouLette  place holder </li>
<li>DBMS  RouLette  SPJ  tuples  SPJ  query plan</li>
</ul>
</li>
<li>RouLette
<ul>
<li>Ingestion Module
<ul>
<li> DBMS  storage engine  table  scan table</li>
<li> vector  vectorized execution </li>
<li> thread</li>
<li> ongoing  incoming  query  table </li>
<li> query scan  table  query  scan </li>
<li> vector  tuple  bit set tuple  query component  query</li>
<li>Scan  round-robin  table query</li>
</ul>
</li>
<li>STeMs
<ul>
<li> in-memory index  table  Eddy Module  plan tree </li>
</ul>
</li>
<li>Eddy
<ul>
<li> episode  global query plan  ongoing query </li>
<li> episode  policy  episode  global plan</li>
<li> selection push-down strategy global plan  selection  join  projection</li>
<li>Join  plan  multi-step optimization (MSO)  policy  RL  episode </li>
<li> operator  query  pairoperator  input  output state  RL </li>
</ul>
</li>
<li>Executor
<ul>
<li> worker thread pool worker  episode episode  table  vector  global query plan</li>
<li>
<ol>
<li> Ingestion  input vector</li>
<li> selection</li>
<li> STeM (hash table)</li>
<li> Symetric Join</li>
<li> tuple  Main DBMS  query plan</li>
</ol>
</li>
<li> vectorized execution</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="core-problems"><a class="header" href="#core-problems">Core Problems</a></h3>
<ul>
<li>How does Eddy generates a global query plan?</li>
<li>How does a worker thread efficently execute the query plan?</li>
</ul>
<h3 id="eddys-global-plan-geneartion-algorithm"><a class="header" href="#eddys-global-plan-geneartion-algorithm">Eddy's Global Plan Geneartion Algorithm</a></h3>
<ol>
<li> input vector</li>
<li> input vector  base relation  join  relation candidates</li>
<li> candidate relation  join</li>
<li> join  relation  join  query set</li>
<li> join  table  selection ( selection  query  constraint )</li>
<li> candidate  join</li>
<li> query join  path  output  query</li>
<li> ( query  join ) candidate  join</li>
<li> query  path </li>
</ol>
<p> candidate </p>
<ul>
<li> query share  join </li>
<li>Join Selectivity
<ul>
<li> relation join  record  match </li>
</ul>
</li>
<li>Join </li>
</ul>
<h3 id="candidate-selection-policy"><a class="header" href="#candidate-selection-policy">Candidate Selection Policy</a></h3>
<p> Eddy  candidate</p>
<ul>
<li>Cost Estimation
<ul>
<li> Operator  cost cost function  input  operator  input  output size</li>
<li> plan  operator  cost  plan  cost</li>
<li>Operator Cost  computation cost linear to input sizeCost function $K_a * n_{in} + \lambda_a * p(o) * n_{in}$ $K_a\lambda_a$ $p(o)$  operator  selectivity
<ul>
<li> opeartor  $K_a\lambda_a$ (join, selection)</li>
</ul>
</li>
</ul>
</li>
<li>Policy Goal:  plan  total cost 
<ul>
<li> goal  RL  total reward </li>
<li> candidate  operator  operator  cost</li>
<li> iterate  possible plan online </li>
</ul>
</li>
<li>RL Modeling
<ul>
<li>State:
<ul>
<li>Virtual vector
<ul>
<li> step  join  relation  query</li>
<li> path ( query) stack  vector</li>
</ul>
</li>
<li>Input size</li>
</ul>
</li>
<li>Action:  path  candidate  operator</li>
<li>Reward:  candidate operator  cost ( join cost  selection cost)</li>
</ul>
</li>
</ul>
<h3 id="opeartor-implementation"><a class="header" href="#opeartor-implementation">Opeartor Implementation</a></h3>
<ul>
<li>Selection
<ul>
<li> tuple  query-set bitmap tuple  query</li>
<li> selection operator  tuple  bitmap tuple  bitmap  bitmap  AND bitmap</li>
<li> predicate evaluation  query  predicate  range range  bitmap binary search  match  range bitmap  tuple </li>
</ul>
</li>
<li>Join
<ul>
<li> SeTMs  joinjoin  bitmap  AND  record</li>
</ul>
</li>
<li>Join Pruning</li>
</ul>
<h2 id="conclusion-8"><a class="header" href="#conclusion-8">Conclusion</a></h2>
<p>Interesting problem and idea, but the in-memory assumption is not realistic.</p>
<h2 id="questions-8"><a class="header" href="#questions-8">Questions</a></h2>
<ul>
<li>Who are using work-sharing? Any practical examples?
<ul>
<li> paper  stream processing batch processing </li>
</ul>
</li>
<li> where  work sharing 
<ul>
<li> Figure 8  where </li>
</ul>
</li>
<li>Section 3  &quot;Ingestion pulls a vector from the hosts storage into RouLette.&quot; vectors
<ul>
<li> vectorized execution</li>
</ul>
</li>
<li>RouLette  Select-Project-Join  case 
<ul>
<li> Select-Project-Join </li>
</ul>
</li>
<li> STeMs  index  in-memory data table Data Warehouse 
<ul>
<li> STeMs  in-memory  RouLette </li>
<li> column store  column</li>
</ul>
</li>
<li> STeMs 
<ul>
<li> RouLette  batch processing  batch  intermediate records (STeMs )</li>
</ul>
</li>
<li> episode  global query plan process  vector of tuples
<ul>
<li> RL  query plan O(1)  time complexity vector  query plan Paper  vector size  1024 episode </li>
</ul>
</li>
<li> query processing  episode query plan 
<ul>
<li> symmetric hash join </li>
</ul>
</li>
<li>RL  cost estimation </li>
</ul>
<h2 id="slides-logics"><a class="header" href="#slides-logics">Slides Logics</a></h2>
<ul>
<li>Background
<ul>
<li>Query Processing
<ul>
<li>parsing query -&gt; optimize query plan -&gt; query execution</li>
</ul>
</li>
<li>Multi-query Processing in OLAP workloads
<ul>
<li> query </li>
<li> query  overlap  record set</li>
</ul>
</li>
<li> query plan </li>
<li>
<ul>
<li>Input: batch of queries</li>
<li>Goal: find a way to execute these queries fast</li>
<li>Assumption: main memory is large enough to fit the working set for the queries</li>
</ul>
</li>
</ul>
</li>
<li>Previous Work
<ul>
<li>Online work-sharing methods
<ul>
<li>QPipe: heuristics to reuse query result or intermediate results</li>
<li>DataPath: detect common sub-plans between multiple queries</li>
<li>CJoin: consider reordering some operators to find common sub-plans
<ul>
<li>TODO: need an example to show why it may not be optimal</li>
</ul>
</li>
</ul>
</li>
<li>Off-line work-sharing methods
<ul>
<li>MQO: iterate all possible query plan to find the optimal global plan for a set of queries.</li>
<li>SWO: similiar to MQO, but considers the frequency of query types.</li>
</ul>
</li>
</ul>
</li>
<li>RouLette
<ul>
<li>Key Idea
<ul>
<li>Global select-join query plan:  query  global query plan select  join </li>
<li>Episodes:  query processing  episodes episodes  tuples global plan
<ul>
<li> Adaptive Query Processing (SIGMOD00 - Eddies)  query  query plan single-query</li>
<li> global query plan</li>
</ul>
</li>
<li>Uses RL to improve global query plan</li>
</ul>
</li>
<li>System Overview (use examples to illustrate)
<ul>
<li>Workflow Graph</li>
<li>Main DBMS
<ul>
<li>Parse queries</li>
<li>Generate a query plan</li>
<li>Take out the select-join sub-plans to the RouLette engine</li>
<li>Wait for the RouLette engine outputs results for further processing</li>
</ul>
</li>
<li>RouLette Engine
<ul>
<li>Ingestion Module: scan each table and keep tracking the progress of scan for each queries
<ul>
<li>Output a vector of tuples for a table (vector size: 1024)</li>
</ul>
</li>
<li>Eddy Module: generate a global query plan
<ul>
<li>Idea: Selection -&gt; Join (selection-push-down)</li>
<li>Steps:
<ul>
<li>Put the selection operator for the tuples first 
<ul>
<li>Filter based on all where constraints</li>
<li>Uses a query-set bitmap</li>
</ul>
</li>
<li>Put insertion operator to a temp table (STeM)</li>
<li>Select a candidate join operator (based on RL)</li>
<li>Final Plan Tree</li>
</ul>
</li>
</ul>
</li>
<li>Executor Module: executes the query plan using a worker thread
<ul>
<li>Can run multiple episodes with multiple worker threads</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>RL Agent to select candidate operators
<ul>
<li>List state, action, reward</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cgptuner-a-contextual-gaussian-process-bandit-approach-for-the-automatic-tuning-of-it-configurations-under-varying-workload-conditions"><a class="header" href="#cgptuner-a-contextual-gaussian-process-bandit-approach-for-the-automatic-tuning-of-it-configurations-under-varying-workload-conditions">CGPTuner: a Contextual Gaussian Process Bandit Approach for the Automatic Tuning of IT Configurations Under Varying Workload Conditions</a></h1>
<ul>
<li>Authors: Stefano Cereda, Stefano Valladares, Paolo Cremonesi, Stefano Doni</li>
<li>Institute:
<ul>
<li>Politecnico di Milano, Milan, Italy</li>
<li>Akamas, Milan, Italy</li>
</ul>
</li>
<li>Published at VLDB'21 (Vol. 14, No. 8)</li>
<li>Paper Link: <a href="http://vldb.org/pvldb/vol14/p1401-cereda.pdf">http://vldb.org/pvldb/vol14/p1401-cereda.pdf</a></li>
</ul>
<h2 id="background-6"><a class="header" href="#background-6">Background</a></h2>
<p>A modern DBMS has hundreds of tunable configurations. Selecting a proper set of configurations is crucial for the performance of the system.</p>
<h2 id="motivation-9"><a class="header" href="#motivation-9">Motivation</a></h2>
<ul>
<li>Hundreds of parameters =&gt; large search space</li>
<li>We also need to consider the parameters of IT stacks (e.g., OS, Java VM) to maximize the performance
<ul>
<li>which means more parameters to tune</li>
</ul>
</li>
<li>The same parameters may also not behave in the same way in different workloads (see Figure 1)</li>
</ul>
<p><img src="dbms_with_ai/cereda2021cgptuner-fg1.PNG" alt="Figure 1" /></p>
<p>Figure 1: Cassandra under different YCSB workloads while varying two configurations</p>
<h2 id="problem-10"><a class="header" href="#problem-10">Problem</a></h2>
<p>Goal: To design a tuning algorithm able to consider the entire IT stack and continuously adapt to the current workload.</p>
<h2 id="previous-work"><a class="header" href="#previous-work">Previous Work</a></h2>
<ul>
<li>Needs to collect data offline
<ul>
<li>iTune
<ul>
<li>Uses Gaussian Processes to approximate the performance surface with different configurations</li>
<li>Con: Learned knowledge cannot be transferred between workloads, which means that we need to rebuild the model for each workload.</li>
</ul>
</li>
<li>OtterTune
<ul>
<li>Has ability to reuse the past experience in other workloads to a new unseen workloads</li>
<li>Con: Requires to collect large amount of data set (over 30k trials per DBMS, about several months)</li>
</ul>
</li>
</ul>
</li>
<li>Online Learning Methods
<ul>
<li>OpenTuner
<ul>
<li>Uses multiple heuristic search algorithm and dynamically selects the best one</li>
<li>Con? Unknown (TODO)</li>
</ul>
</li>
<li>BestConfig
<ul>
<li>Iterative sampling strategy</li>
<li>Con? Unknown (TODO)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="method-10"><a class="header" href="#method-10">Method</a></h2>
<h3 id="problem-modeling"><a class="header" href="#problem-modeling">Problem Modeling</a></h3>
<p>A contextual bandit problem:</p>
<ul>
<li>Inputs (Context):
<ul>
<li>The current workload \(\vec{w}_i \in W\)</li>
</ul>
</li>
<li>Output (Action):
<ul>
<li>The configurations of the IT stack \(\vec{x}_i \in X\)</li>
</ul>
</li>
<li>The response of the system (Reward):
<ul>
<li>Certain performance indicator (e.g., throughput, latency...) \(y_i \in \mathbb{R}\)</li>
</ul>
</li>
</ul>
<p>Workflow:</p>
<p><img src="dbms_with_ai/cereda2021cgptuner-fg2.PNG" alt="Figure 2" /></p>
<h3 id="main-idea-2"><a class="header" href="#main-idea-2">Main Idea</a></h3>
<p>Bayesian Optimization using Gaussian Processes:</p>
<ul>
<li>The regression model: multi-variate Gaussian distributions</li>
<li>Kernel: \(k((\vec{x}, \vec{w}), (\vec{x}', \vec{w}')) = k(\vec{x}, \vec{x}') + k(\vec{w}, \vec{w}')\)
<ul>
<li>where \(k(a, a')\) is Matrn 5/2 kernel for both \(a = \vec{x}\) and \(\vec{w}\)</li>
</ul>
</li>
<li>The acquisition function: the GP-Hedge method</li>
</ul>
<p>Key Steps:</p>
<ol>
<li>Sample a function \( f_{\vec{w}_i}\) using Gaussian Processes with previous observations \( (\vec{x}_n, \vec{w}_n, y_n) \) for \( n = 0 ... i - 1 \) and current workload \(\vec{w}_i\)</li>
<li>Using the acquisition function \( a(.) \) to optimize \( max_\vec{x} a(f_{\vec{w}_i}, \vec{x}) \) to obtain best \( \vec{x} \)</li>
</ol>
<h3 id="normalizing-performance"><a class="header" href="#normalizing-performance">Normalizing Performance</a></h3>
<p>In order to avoid bad exploration due to zero mean sample far away from previous observation, they found that, instead of directly using the performance indicator \( y_i \), we should use Normalized Performance Improvement (NPI):</p>
<p><img src="dbms_with_ai/cereda2021cgptuner-fm3.PNG" alt="Formula 3" /></p>
<p>where \( \vec{x}^+_\vec{w} \) is the best configuration we have seen so far.</p>
<p>NPI must be re-normalized after each iteration since the best configuration may change.</p>
<h2 id="experiments-6"><a class="header" href="#experiments-6">Experiments</a></h2>
<h2 id="conclusion-9"><a class="header" href="#conclusion-9">Conclusion</a></h2>
<h2 id="questions-9"><a class="header" href="#questions-9">Questions</a></h2>
<ul>
<li>What does <code>vm.dirty_ratio</code> do?</li>
<li>It seems like OpenTuner has already used multi-armed bandits to solve tuning problems. What are the differences between it and this work?</li>
</ul>
<h2 id="background-knowledge"><a class="header" href="#background-knowledge">Background Knowledge</a></h2>
<p>TODO</p>
<ul>
<li>Baysian Optimization</li>
<li>Gaussain Processes</li>
<li>GP-Hedge methods</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="one-model-to-rule-them-all-towards-zero-shot-learning-for-databases"><a class="header" href="#one-model-to-rule-them-all-towards-zero-shot-learning-for-databases">One Model to Rule them All: Towards Zero-Shot Learning for Databases</a></h1>
<ul>
<li>Authors: Benjamin Hilprecht, Carsten Binnig</li>
<li>Institute: Technical University of Darmstadt, Germany</li>
<li>Published at CIDR'22</li>
<li>Paper Link: <a href="http://cidrdb.org/cidr2022/papers/p16-hilprecht.pdf">http://cidrdb.org/cidr2022/papers/p16-hilprecht.pdf</a></li>
</ul>
<h2 id="background-7"><a class="header" href="#background-7">Background</a></h2>
<p>There have been many new proposals for using AI to improve DBMS components such as:</p>
<ul>
<li>More accurate cost estimators</li>
<li>Faster query optimizers</li>
<li>DBMS auto configurations (parameter tuning and physical design)</li>
</ul>
<h2 id="motivation-10"><a class="header" href="#motivation-10">Motivation</a></h2>
<p>Those learning-based techniques usually require retraining once we apply them to a new database or workload, which may require great effort to collect training data.</p>
<h2 id="problem-11"><a class="header" href="#problem-11">Problem</a></h2>
<p>This paper aims to propose several techniques that help learning-based methods perform well when they are transferred to a new database without retraining.</p>
<h2 id="method-11"><a class="header" href="#method-11">Method</a></h2>
<h3 id="key-insights"><a class="header" href="#key-insights">Key Insights</a></h3>
<p>There are a few insights that may be the keys toward zero-shot learning techniques:</p>
<ul>
<li>Transferable representations of database and queries
<ul>
<li>In order to make a learning-based technique transferrable across databases, the format of representations should not depend on databases and queries.</li>
</ul>
</li>
<li>Training data collection and robustness
<ul>
<li>How to collect effective training data is important to enable zero-shot learning</li>
<li>A preliminary experiment shows that we can use a relative small training data set to outperform the state of the art when transferring from one database to another.</li>
<li>We need a way to guide us to find more effective training samples</li>
</ul>
</li>
<li>Separation of concerns
<ul>
<li>Decomposing a end-to-end model into smaller task-specific models help these small models more transferable across databases.</li>
</ul>
</li>
</ul>
<h3 id="transferable-representations"><a class="header" href="#transferable-representations">Transferable Representations</a></h3>
<p><img src="dbms_with_ai/hilprecht2022zeroshot-figure1.png" alt="Transferable Representations" /></p>
<p>Two key techniques:</p>
<ul>
<li>Graph Encoding
<ul>
<li>Modeling plan operator, predicates, table names as graph nodes</li>
</ul>
</li>
<li>Transferable Featurization
<ul>
<li>Using general statistics instead of database-specific features, such as number of tuples and pages.</li>
</ul>
</li>
</ul>
<h2 id="experiments-7"><a class="header" href="#experiments-7">Experiments</a></h2>
<p>The initial results show that their models outperforms the state of the art models on an unseen database (IMDB) without additional retraining.</p>
<h2 id="conclusion-10"><a class="header" href="#conclusion-10">Conclusion</a></h2>
<ul>
<li>Interesting direction</li>
<li>Needs more descriptions on how to feature queries and other case studies</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="balsa-learning-a-query-optimizer-without-expert-demonstrations-wip"><a class="header" href="#balsa-learning-a-query-optimizer-without-expert-demonstrations-wip">Balsa: Learning a Query Optimizer Without Expert Demonstrations (WIP)</a></h1>
<ul>
<li>Authors: Zongheng Yang, Wei-Lin Chiang, Sifei Luan, Gautam Mittal, Michael Luo, Ion Stoica</li>
<li>Institute: UC Berkeley</li>
<li>Published at SIGMOD'22</li>
<li>Paper Link: <a href="https://arxiv.org/abs/2201.01441">https://arxiv.org/abs/2201.01441</a></li>
</ul>
<h2 id="background-8"><a class="header" href="#background-8">Background</a></h2>
<p>Query optimizer is an important component that finds out a good execution plan for a query.</p>
<h2 id="motivation-11"><a class="header" href="#motivation-11">Motivation</a></h2>
<h3 id="dbms-without-good-optimizer"><a class="header" href="#dbms-without-good-optimizer">DBMS without Good Optimizer</a></h3>
<p> PostgreSQL  DBMS  Query Optimizer NewSQL  Query Optimizer  Query Optimizer </p>
<p> ML Model </p>
<h3 id="rl-without-demonstration"><a class="header" href="#rl-without-demonstration">RL without Demonstration</a></h3>
<p> RL  Query Optimizer  Query Optimizer  optimizer </p>
<p> ( optimizer)  RL </p>
<h2 id="problem-12"><a class="header" href="#problem-12">Problem</a></h2>
<p> expert optimizer  RL  query optimizer</p>
<h3 id="challenges-2"><a class="header" href="#challenges-2">Challenges</a></h3>
<ul>
<li> expert optimizer exploration exploration 
<ul>
<li> plan </li>
<li> plan</li>
</ul>
</li>
</ul>
<h3 id="assumptions-4"><a class="header" href="#assumptions-4">Assumptions</a></h3>
<ul>
<li>Database </li>
<li> query plan  select-project-join block </li>
</ul>
<h2 id="previous-work-1"><a class="header" href="#previous-work-1">Previous Work</a></h2>
<ul>
<li>DQ:  query optimizer  cost model performance  cost model </li>
<li>Neo:  query optimizer  plan </li>
</ul>
<p> expert cost model  optimizer</p>
<h2 id="method-12"><a class="header" href="#method-12">Method</a></h2>
<p> paper  simulation-to-reality learning simulated  simulation  query plan</p>
<p> insight simulator  plan  simulation  label </p>
<h3 id="simulator"><a class="header" href="#simulator">Simulator</a></h3>
<p>Balsa  simulatorinput  query planoutput  cost cost  operator  output records  operator  cost  </p>
<p><img src="dbms_with_ai/yang2022balsa-formula-cost.png" alt="Cost Model" /></p>
<p> |T|  cardinality estimator  PostgreSQL  estimator</p>
<p> paper </p>
<ul>
<li>Join  hash join</li>
<li>Join  selectivity  1</li>
<li> model join  scan  cost</li>
</ul>
<p> simulator  simulator  agent  79  plan  5.8  plan</p>
<p> Balsa  Selinger-style optimization  query  query plan query plan  sub plan  cost  data set </p>
<p> Balsa train  neural network \(V_{sim}\): (query, sub-plan) -&gt; (cost)</p>
<h3 id="real-execution"><a class="header" href="#real-execution">Real Execution</a></h3>
<p>Balsa  train  neural network \(V_{real}\): (query, sub-plan) -&gt; (overall latency)</p>
<p></p>
<ul>
<li>\(V_{real}\)  \(V_{sim}\)  label  latency  cost \(V_{real}\)  \(V_{sim}\)  weights </li>
<li>\(V_{real}\)  overall latency  sub-plan  query  latency sub-plan  query  latency</li>
</ul>
<p> training</p>
<ol>
<li> query</li>
<li> Balsa  plan search  \(V_{real}\)  k  query plans</li>
<li> k  query plans  query plan latency</li>
<li> query plan  sub-plan  overall latency  data set </li>
<li> SGD  L2 Loss  \(V_{real}\)</li>
</ol>
<h3 id="plan-search"><a class="header" href="#plan-search">Plan Search</a></h3>
<p>Plan search  NLP  beam search</p>
<p>Beam search  priority queue b Balsa  plan tree  state  queue  beam search</p>
<ol>
<li> query  table  state: {T1, T2, T3...}</li>
<li> state  queue </li>
<li>Beam search  queue  pop  state</li>
<li> state  intermediate state
<ul>
<li> join  sub-plan/tables</li>
<li>Join  operator  (hash, merge...)</li>
</ul>
</li>
<li> intermediate state  sub-plan  \(V_{real}\)  overall latency state  latency</li>
<li> state  queuequeue  latency </li>
<li> 3  k  query plan</li>
</ol>
<h3 id="some-tricks"><a class="header" href="#some-tricks">Some Tricks</a></h3>
<p> training </p>
<ul>
<li>On-policy learning:  \(V_{real}\)  data point  \(V_{real}\)</li>
<li>Timeout Policy:  query plan  execution </li>
<li>Count-based exploration:  beam search  top-k  plan  exploration  diversity</li>
<li>Diversified experiences: Balsa  N  agent random seed N  agent  data set  agent experience  diversity agent  generalizability</li>
</ul>
<h2 id="experiments-8"><a class="header" href="#experiments-8">Experiments</a></h2>
<h2 id="conclusion-11"><a class="header" href="#conclusion-11">Conclusion</a></h2>
<h2 id="questions-10"><a class="header" href="#questions-10">Questions</a></h2>
<ul>
<li> RL  expert demonstration </li>
<li> cost estimator learn  estimator model fine tune  model plan  algorithm  learn  estimator model </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tastes-great-less-filling-high-performance-and-accurate-training-data-collection-for-self-driving-database-management-systems"><a class="header" href="#tastes-great-less-filling-high-performance-and-accurate-training-data-collection-for-self-driving-database-management-systems">Tastes Great! Less Filling! High Performance and Accurate Training Data Collection for Self-Driving Database Management Systems</a></h1>
<ul>
<li>Authors: Matthew Butrovich, Wan Shen Lim, Lin Ma, John Rollinson, William Zhang, Yu Xia, Andrew Pavlo</li>
<li>Institute: Carnegie Mellon University, Army Cyber Institute, Massachusetts Institute of Technology</li>
<li>Published at SIGMOD'22</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/10.1145/3514221.3517845">https://dl.acm.org/doi/10.1145/3514221.3517845</a></li>
</ul>
<h2 id="background-9"><a class="header" href="#background-9">Background</a></h2>
<p>A self-driving DBMS usually contains a behavior modeling module which predicts the cost of a database action on a given workload.</p>
<p>The module needs a set of training data to train, so the system needs a method to collect these data.</p>
<h2 id="motivation-12"><a class="header" href="#motivation-12">Motivation</a></h2>
<p>Current training data collection scheme:</p>
<ul>
<li>Offline
<ul>
<li>Method 1: Cloning a database and simulate an existing workload trace
<ul>
<li>Con:
<ul>
<li>Cloning a database is time-consuming</li>
<li>Recording workload trace is also not easy</li>
</ul>
</li>
</ul>
</li>
<li>Method 2: Running a new database with synthetic queries
<ul>
<li>Con:
<ul>
<li>Needs extra time for simulating workloads (maybe days or weeks) to generate enough data for robustness</li>
<li>Cannot capture the real metrics of online environments</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Online
<ul>
<li>Con: overhead too high</li>
</ul>
</li>
</ul>
<p>Needs an online method with low overhead</p>
<h2 id="problem-13"><a class="header" href="#problem-13">Problem</a></h2>
<h3 id="requirements"><a class="header" href="#requirements">Requirements</a></h3>
<ul>
<li>Needs a method that collects <strong>internal</strong> features (CPU time, # of concurrent workers, info of GC etc.)
<ul>
<li>External features are not accurate</li>
</ul>
</li>
<li>Needs a method that collects metrics in <strong>kernel-space</strong>.
<ul>
<li>Collecting metrics in user-space is expensive due to the overhead of system calls and I/O</li>
</ul>
</li>
</ul>
<h2 id="method-13"><a class="header" href="#method-13">Method</a></h2>
<ul>
<li>How to collect metrics in kernel-space with low overhead?
<ul>
<li>By writing a kernel module using Berkley Packet Filter (BPF) library, which allows a user to write a kernel module without knowing much kernel knowledge.
<ul>
<li>Pro:
<ul>
<li>Has OS-level privilege</li>
<li>No need to run DBMS using root privilege</li>
<li>Faster</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="experiments-9"><a class="header" href="#experiments-9">Experiments</a></h2>
<h2 id="conclusion-12"><a class="header" href="#conclusion-12">Conclusion</a></h2>
<h2 id="questions-11"><a class="header" href="#questions-11">Questions</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dbms-experiments"><a class="header" href="#dbms-experiments">DBMS Experiments</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vldb22---a-study-of-database-performance-sensitivity-to-experiment-settings"><a class="header" href="#vldb22---a-study-of-database-performance-sensitivity-to-experiment-settings">VLDB'22 - A Study of Database Performance Sensitivity to Experiment Settings</a></h1>
<ul>
<li>Authors: Yang Wang, Miao Yu, Yujie Hui, Fang Zhou, Yuyang Huang, Rui Zhu, Xueyuan Ren, Tianxi Li, Xiaoyi Lu</li>
<li>Institute: The Ohio State University</li>
<li>Published at VLBD'22</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/abs/10.14778/3523210.3523221">https://dl.acm.org/doi/abs/10.14778/3523210.3523221</a></li>
</ul>
<h2 id="background-10"><a class="header" href="#background-10">Background</a></h2>
<p>Many DBMS articles compare their systems with other baselines using the TPC-C benchmarks or YCSB, but these benchmarks have many tunable parameters.</p>
<h2 id="motivation--problem"><a class="header" href="#motivation--problem">Motivation &amp; Problem</a></h2>
<p>This paper tries to find out &quot;how sensitive are their evaluation results to these parameters and will their conclusions hold under a different setting?&quot;</p>
<h2 id="method-14"><a class="header" href="#method-14">Method</a></h2>
<h3 id="reproduced-systems"><a class="header" href="#reproduced-systems">Reproduced Systems</a></h3>
<p>They use the source code from the original paper to reproduce the system. A system is considered as reproduced as long as:</p>
<ol>
<li>The reproduced numbers are reasonably close to those in the corresponding article or have an explainable deviation</li>
<li>the conclusion of the article still holds</li>
</ol>
<p>Reproduced systems list:</p>
<ul>
<li>Transactional DBMSs
<ul>
<li>Focusing on multi-core single-machine
<ul>
<li>Silo</li>
<li>Cicada</li>
</ul>
</li>
<li>Focusing on distributed transactions
<ul>
<li>Using RDMA
<ul>
<li>DrTM</li>
<li>GAM</li>
</ul>
</li>
<li>Without using RDMA
<ul>
<li>Focusing on geo-distributed databases
<ul>
<li>TAPIR</li>
<li>Janus</li>
</ul>
</li>
<li>Not especially focusing on geo-distributed env.
<ul>
<li>Calvin</li>
<li>Star</li>
<li>Aria</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Key-value DBMSs
<ul>
<li>HERD</li>
<li>MICA</li>
</ul>
</li>
</ul>
<h2 id="experiments-10"><a class="header" href="#experiments-10">Experiments</a></h2>
<h3 id="the-tpc-c-benchmarks"><a class="header" href="#the-tpc-c-benchmarks">The TPC-C Benchmarks</a></h3>
<p>They tried to answer the following questions:</p>
<ul>
<li>Considering commercial systems are tested with <strong>wait time</strong> and research prototypes are not, how much difference does it make?</li>
<li>Comparing to commercial systems, most research prototypes are tested with a small number of warehouses. What is the impact of <strong>the number of warehouses</strong>?</li>
<li>Different prototypes are evaluated under different degrees of concurrency. Will their conclusions still hold under a different <strong>degree of concurrency</strong>?</li>
<li>What is the difference between running transactions as <strong>stored procedures</strong> and running them as interactive SQL transactions?</li>
<li>Considering some research prototypes only run two types of transactions (i.e. New-Order and Payment), what is <strong>the impact of the remaining three types</strong>?</li>
</ul>
<h4 id="impact-of-wait-time"><a class="header" href="#impact-of-wait-time">Impact of Wait Time</a></h4>
<p>Key Findings</p>
<ul>
<li>Due to the restriction of wait time (21 seconds in average) and number of clients (10) to each warehouse, the upper bound of expected throughput is 0.48 txns/sec per warehouse.</li>
<li>With wait time
<ul>
<li>Almost no contention</li>
<li>Bottleneck becomes I/O</li>
</ul>
</li>
<li>Most systems focus on improving performance under contention, so it is not suitable for them to use wait time.</li>
</ul>
<h4 id="impact-of-contention-level"><a class="header" href="#impact-of-contention-level">Impact of Contention Level</a></h4>
<p>Key factors for contention</p>
<ul>
<li>Number of warehouses (with a fixed number of clients)</li>
<li>Number of clients (with a fixed number of warehouses)</li>
<li>Percentage of cross-warehouse transactions
<ul>
<li>Cross-warehouse transactions have higher chance to contend with more transactions</li>
</ul>
</li>
</ul>
<p>Key findings</p>
<ul>
<li>Whether adjusting contention level by changing the number of warehouses work depends on if contention is a bottleneck (check by if the CPU is underutilized)
<ul>
<li>If contention is a bottleneck, it does work.</li>
<li>If contention is not a bottleneck (e.g., Silo, DrTM, and GAM), increasing the number of warehouses causes the throughput decreases.</li>
</ul>
</li>
<li>Many systems are sensitive to the number of warehouses. This may change the conclusions of the papers.
<ul>
<li>E.g., Aria outperforms Calvin in low contention environment, but not in high contention env.</li>
</ul>
</li>
</ul>
<h4 id="impact-of-network-and-disk-io"><a class="header" href="#impact-of-network-and-disk-io">Impact of Network and Disk I/O</a></h4>
<p>Key solutions to avoid I/O impact throughput:</p>
<ul>
<li>Faster I/O: using fast hardware and mechanisms, e.g., RDMA</li>
<li>Smart scheduling: e.g., early lock release</li>
<li>Separate I/O: e.g., Calvin decouples replicating transactions from transaction execution</li>
</ul>
<p>Experiments (Figure 3) show that I/O impacts scalability.</p>
<h4 id="impact-of-not-using-stored-procedures"><a class="header" href="#impact-of-not-using-stored-procedures">Impact of Not using Stored Procedures</a></h4>
<p>Challenges of using interactive SQL statements:</p>
<ul>
<li>Hard to know read-/write-set in advance</li>
<li>Adding more network round-trips</li>
<li>Adding overhead of parsing SQLs</li>
</ul>
<p>Key findings (Figure 4)</p>
<ul>
<li>Using stored procedures is much faster</li>
<li>However, increasing concurrency level (e.g., increasing number of warehouses) can hide the above cost since it is easier to pipeline transactions.</li>
</ul>
<p>Finding read-/write-set from interactive SQL statements is an interesting research direction.</p>
<h4 id="impact-fo-transaction-types"><a class="header" href="#impact-fo-transaction-types">Impact fo Transaction Types</a></h4>
<p>Key findings (Figure 5)</p>
<ul>
<li>Using only New-Order and Payment can singnificantly increase throughput to most of systems</li>
<li>However, it is not the case for Janus because
<ul>
<li>the bottleneck of Janus is not CPU</li>
<li>New-Order and Payment may have cross-warehouse behavior which increases the overhead</li>
</ul>
</li>
</ul>
<h4 id="summary-of-tpc-c"><a class="header" href="#summary-of-tpc-c">Summary of TPC-C</a></h4>
<p>Tuning guidelines</p>
<ul>
<li>Test disk throughput =&gt; ensure data does not fit into DRAM</li>
<li>Test network stack =&gt; run interactive transactions, 2PC, Paxos with low contention (many warehouses, but fit into DRAM)</li>
<li>Test concurrency control =&gt; high contention (low number of warehouses)</li>
</ul>
<h3 id="the-yahoo-cloud-serving-benchmarks-ycsb"><a class="header" href="#the-yahoo-cloud-serving-benchmarks-ycsb">The Yahoo! Cloud Serving Benchmarks (YCSB)</a></h3>
<p>They tried to answer the following questions:</p>
<ul>
<li>Since some systems have network stacks and some do not, how does network stack affect the result?</li>
<li>How does the skewness of keys in the workload affect the result?</li>
<li>How do the number and size of KV pairs affect the result?</li>
<li>How does the read/write ratio affect the result?</li>
</ul>
<h4 id="impact-of-network-stack"><a class="header" href="#impact-of-network-stack">Impact of Network Stack</a></h4>
<p>Key findings:</p>
<ul>
<li>(Table 3) Network stack efficiency: RDMA &gt; DPDK &gt; TCP/UDP</li>
<li>(Figure 6d) Large KVs with RDMA may be slow for reading records since reading requires using RDMA SEND, which is slower than RDMA WRITE.</li>
</ul>
<h4 id="impact-of-skewness"><a class="header" href="#impact-of-skewness">Impact of Skewness</a></h4>
<p>Key findings:</p>
<ul>
<li>(Figure 6a, 6b) High skewness makes the systems having concurrency control slower.
<ul>
<li>(Figure 6d, 6e) But not for the systems that do not need currency control (H-Store-like design), it only causes load imbalances. (Note that there is no transaction in YCSB)</li>
</ul>
</li>
</ul>
<h4 id="impact-of-number-of-kvs"><a class="header" href="#impact-of-number-of-kvs">Impact of Number of KVs</a></h4>
<p>Key findings:</p>
<ul>
<li>(Figure 6a, 6b) the impact of number of KVs is not significant.
<ul>
<li>(Figure 7) This is due to the design of Zipfian distribution. Even if raising the number of KVs from 1M to 100M, the frequency of accessing the hottest key only decreases from 6.5% to 4.8%.</li>
</ul>
</li>
</ul>
<h4 id="impact-of-readwrite-ratio"><a class="header" href="#impact-of-readwrite-ratio">Impact of Read/Write Ratio</a></h4>
<p>Key findings:</p>
<ul>
<li>(Figure 6a, 6c) more writes =&gt; slower
<ul>
<li>since reads can be processed concurrently</li>
</ul>
</li>
<li>Some systems that need to replicate writes (e.g., Star) gets higher impact with higher write ratio.</li>
</ul>
<h4 id="summary-of-ycsb"><a class="header" href="#summary-of-ycsb">Summary of YCSB</a></h4>
<p>Tuning guidelines</p>
<ul>
<li>Test network bandwidth =&gt; using large KVs with low contention</li>
<li>Test network stack =&gt; using small KVs with low contention</li>
<li>Test KV lookup speed =&gt; using small KVs with low contention but batching requests</li>
<li>Test concurrency control =&gt; high contention</li>
</ul>
<h2 id="conclusion-13"><a class="header" href="#conclusion-13">Conclusion</a></h2>
<h2 id="questions-12"><a class="header" href="#questions-12">Questions</a></h2>
<ul>
<li>How does Janus decouple I/O from critical sections?</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reinforcement-learning-to-rank"><a class="header" href="#reinforcement-learning-to-rank">Reinforcement Learning to Rank</a></h1>
<ul>
<li><a href="rl_to_rank/zhou2020rlirank.html">WWW'20 - RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rlirank-learning-to-rank-with-reinforcement-learning-for-dynamic-search"><a class="header" href="#rlirank-learning-to-rank-with-reinforcement-learning-for-dynamic-search">RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search</a></h1>
<ul>
<li>Authors: Jianghong Zhou, Eugene Agichtein</li>
<li>Institute: Emory University, Atlanta, USA</li>
<li>Published at WWW'20</li>
<li>Paper Link: <a href="https://dl.acm.org/doi/10.1145/3366423.3380047">https://dl.acm.org/doi/10.1145/3366423.3380047</a></li>
</ul>
<h2 id="background-11"><a class="header" href="#background-11">Background</a></h2>
<p>Dynamic search is an iterative process to rank documents and collect feedbacks from a user in order to come out the best ranking that fits the query provided by the user.</p>
<h2 id="motivation-13"><a class="header" href="#motivation-13">Motivation</a></h2>
<p>They claim that the previous work that uses learning to rank (LTR) methods fail to capture all the ranked documents' information to improve the overall quality of ranking.</p>
<h2 id="problem-14"><a class="header" href="#problem-14">Problem</a></h2>
<p>To design a RL agent that ranks documents iteratively for dynamic search.</p>
<h2 id="method-15"><a class="header" href="#method-15">Method</a></h2>
<h3 id="rl-modeling"><a class="header" href="#rl-modeling">RL Modeling</a></h3>
<p>State:</p>
<ul>
<li>A sequence of (\(d\), \(q\)) pairs. 
<ul>
<li>\(d\): the embedded vector of a ranked document</li>
<li>\(q\): the embedded vector of the current query</li>
</ul>
</li>
</ul>
<p>Action:</p>
<ul>
<li>\(a_r\): a picked document
<ul>
<li>This action updates the state by adding the picked document to the sequence</li>
</ul>
</li>
<li>\(a_t\): the action to update the query by the feedback from the user
<ul>
<li>This action updates the state by replacing all the queries with the new query</li>
</ul>
</li>
</ul>
<p>Reward: NDCG or \(\alpha\)-NDCG</p>
<p>RL Method:</p>
<ul>
<li>Choosing the action with the max expected reward (not total reward).</li>
<li>The expected reward </li>
</ul>
<h3 id="document-and-query-embedding"><a class="header" href="#document-and-query-embedding">Document and Query Embedding</a></h3>
<p>Uses Google Universal Sentence Encoder</p>
<h2 id="experiments-11"><a class="header" href="#experiments-11">Experiments</a></h2>
<p>Looks great. However, it shows this method outperforms MDP method. Why?</p>
<h2 id="conclusion-14"><a class="header" href="#conclusion-14">Conclusion</a></h2>
<p>This is definitely not a usual RL approach. I am not sure why it works.</p>
<h2 id="questions-13"><a class="header" href="#questions-13">Questions</a></h2>
<ul>
<li>Why did it use stacked RNN?</li>
<li>What is the value network presented in the paper? Is that a DQN method?
<ul>
<li>It seems like &quot;the value network&quot; is a network to predict the reward given the current state and action. So, it is not a DQN method.</li>
</ul>
</li>
<li>It uses NDCG to calculate the rewards. What is that?</li>
<li>Why is its loss function to minimize the relevance scores? Why not just maximizing rewards?</li>
<li>What is MDP in the experiments? Does it mean Markov Decision Process?</li>
<li>\(a_t\) depends on user's feedbacks. How does the RL agent iterate each action before it receives user's feedbacks?</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
                
    </body>
</html>
